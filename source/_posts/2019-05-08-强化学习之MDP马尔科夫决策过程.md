---
title: 强化学习之MDP马尔科夫决策过程
copyright: true
top: 1
date: 2019-05-08 11:04:20
mathjax: true
categories: ReinforcementLearning
tags:
- rl
---

#强化学习之MDP马尔科夫决策过程

每每提到强化学习，最先接触的理论肯定是马尔科夫决策过程（MDP，Markov Decision Process），为什么总提到MDP呢？并不是只有我一个人有这个疑问。

百度上没有人提出这样的问题，可能是大家理解得都比较透彻吧，于是在Google查到相关提问和解释。

> [What is the relationship between Markov Decision Processes and Reinforcement Learning?](https://datascience.stackexchange.com/a/38851)

>>In Reinforcement Learning (RL), the problem to resolve is described as a Markov Decision Process (MDP). Theoretical results in RL rely on the MDP description being a correct match to the problem. If your problem is well described as a MDP, then RL may be a good framework to use to find solutions. That does not mean you need to fully describe the MDP (all the transition probabilities), just that you expect an MDP model could be made or discovered.

>>Conversely, if you cannot map your problem onto a MDP, then the theory behind RL makes no guarantees of any useful result.

>>One key factor that affects how well RL will work is that the states should have the Markov property - that the value of the current state is enough knowledge to fix immediate transition probabilities and immediate rewards following an action choice. Again you don't need to know in advance what those are, just that this relationship is expected to be reliable and stable. If it is not reliable, you may have a POMDP. If it is not stable, you may have a non-stationary problem. In either case, if the difference from a more strictly defined MDP is small enough, you may still get away with using RL techniques or need to adapt them slightly.

MDP是当前强化学习理论推导的基石，对强化学习来说，一般以马尔科夫决策过程作为形式化问题的手段。也就是说，对于目前的绝大部分强化学习算法，只有可以将问题抽象为MDP的才可以确保算法的性能（收敛性，效果等），对于违背MDP的问题并不一定确保算法有效，因为其数学公式都是基于MDP来进行推导的。

## 马尔科夫性

> 马尔科夫性质（英语：Markov property）是概率论中的一个概念，因为俄国数学家安德雷·马尔科夫得名。当一个随机过程在给定现在状态及所有过去状态情况下，其未来状态的条件概率分布仅依赖于当前状态；换句话说，在给定现在状态时，它与过去状态（即该过程的历史路径）是条件独立的，那么此随机过程即具有马尔科夫性质。[马尔科夫性-百度百科](https://baike.baidu.com/item/%E9%A9%AC%E5%B0%94%E5%8F%AF%E5%A4%AB%E6%80%A7%E8%B4%A8/23149887?fr=aladdin)

马尔科夫性，也就是无后效性：**某阶段的状态一旦确定，则此后过程的演变不再受此前各状态及决策的影响**。也就是说，**未来与过去无关**。

具体地说，如果一个问题被划分各个阶段之后，阶段$k$中的状态只能通过阶段$k+1$中的状态通过状态转移方程得来，与其他状态没有关系，特别是与未发生的状态没有关系，这就是无后效性。

公式描述：

$$
P[S_{t+1}|S_{t}]=P[S_{t+1}|S_{1},...,S_{t}]
$$

强化学习问题中的状态也符合马尔科夫性，即在当前状态$s_{t}$下执行动作$a_{t}$并转移至下一个状态$s_{t+1}$，而不需要考虑之前的状态$s_{t-1},...,s_{1}$。

举一个不恰当的例子：

![](/2019/05/08/强化学习之MDP马尔科夫决策过程/M.jpg)

假设天气预测符合马尔科夫性，如果以每天表示为一种状态，即周一、周二到周日。今天（5月8日，周三）天气为晴，明天（周四）会不会下雨只与今天的天气有关，而与之前周一、周二的天气状况无关。如果以时间节点表示为一种状态，即2点、5点、8点等，如图2点的温度为15.8°C,那么下个时间点5点的气温如何只与2点的温度有关系。

强化学习中默认状态的转移是符合马尔科夫性质的，状态具体是什么，需要根据不同的问题进行不同的设定。

## 马尔科夫过程

马尔科夫过程是随机过程的一种，什么是随机过程呢？简单来说，一个商店从早上营业到晚上打烊这段时间，根据每个时间点店内顾客的人数所组成的序列就是随机过程。随机过程根据时间节点$T_{t}$取到的值是一个变量。

马尔科夫过程是满足马尔科夫性的随机过程，它由二元组$M=(S,P)$组成，且满足：

1. S是有限状态集合
2. P是状态转移概率矩阵

状态与状态之间的转换过程即为马尔科夫过程。***虽然我们可能不知道P的具体值到底是什么，但是通常我们假设P是存在的（转移概率存在，如果是确定的，无非就是概率为1），而且是稳定的（意思是从状态A到其他状态的转移虽然符合某个分布，但是其转移到某个状态的概率是确定的，不随时间变化的）。***

这里说的**有限**二字我有自己的理解，在最开始的强化学习研究中，解决的都是表格式的问题，也就是状态的数量是有限可取的，但是后续强化学习研究的也有连续状态空间的问题，算法如DQN,PG,PPO等。状态的数量并不是有限的，但是其向量维度则是固定的、有限的，而且也同样符合马尔科夫性质，因此**我认为这里定义的有限并不是说状态数量有限，而是状态维度有限**。

### 状态转移矩阵

状态转移矩阵由许多状态转移概率组成，状态转移概率是指从一个马尔科夫状态$s$转移到下一个状态$s'$的概率。

公示表示：

$$
\mathcal{P}_{ss'}=\mathbb{P}[S_{t+1}=s'|S_{t}=s]
$$

等同于：

$$
\mathcal{P}(s'|s)=\mathbb{P}[S_{t+1}=s'|S_{t}=s]
$$

假设有1到n个状态，将所有的状态从上到下、从左到右排列，组成一个$n \times n$的矩阵，那么其状态转移矩阵如下所示：

$$
\mathcal{P}=
\begin{bmatrix}
\mathcal{P}_{11} & \cdots & \mathcal{P}_{1n} \\ 
\vdots & \ddots & \vdots \\ 
\mathcal{P}_{n1} & \cdots & \mathcal{P}_{nn} \\
\end{bmatrix}
$$

其中，每行元素相加等于1，每列元素相加等于1，矩阵的总和为状态的数量n。

对于可数状态，$\sum_{s'=1}^{n}\mathcal{P}(s'|s)=1$

$$
sum(\mathcal{P}) = \sum_{s'=1}^{n}\sum_{s=1}^{n}\mathcal{P}_{ss'} = n
$$

对于不可数状态（连续状态),$\int_{s'}\mathcal{P}(s'|s)=1$

$$
sum(\mathcal{P}) = \int_{s'}\int_{s}\mathcal{P}_{ss'} = n
$$




