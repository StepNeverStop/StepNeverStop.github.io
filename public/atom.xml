<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Keavnn&#39;Blog</title>
  
  <subtitle>If it is to be, it is up to me.</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://StepNeverStop.github.io/"/>
  <updated>2020-06-23T13:59:09.263Z</updated>
  <id>http://StepNeverStop.github.io/</id>
  
  <author>
    <name>Keavnn</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>Exploration By Random Network Distillation</title>
    <link href="http://StepNeverStop.github.io/random-network-distillation.html"/>
    <id>http://StepNeverStop.github.io/random-network-distillation.html</id>
    <published>2020-06-23T12:50:48.000Z</published>
    <updated>2020-06-23T13:59:09.263Z</updated>
    
    <content type="html"><![CDATA[<p>这篇论文提出了RND，是一种新奇的内在探索机制，主要原理是使预测网络的特征逼近随机网络的特征，以此使得预测网络对不熟悉的状态给出较大的预测误差，由此设定内在奖励。</p><p>推荐程度中等：</p><ul><li>1个额外的固定、不训练的随机网络</li><li>RND在强探索（hard exploration，奖励很稀疏）环境中表现不错</li></ul><a id="more"></a><h1 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h1><p>论文地址：<a href="http://arxiv.org/abs/1810.12894" rel="external nofollow" target="_blank">http://arxiv.org/abs/1810.12894</a></p><p>代码地址：<a href="https://github.com/openai/random-network-distillation" rel="external nofollow" target="_blank">https://github.com/openai/random-network-distillation</a></p><p>这篇论文的思想特别简单：<strong>设定一个固定不训练的随机网络和一个训练的预测网络，将同一个图像特征分别放入两个网络计算出特征，两个特征的差异程度就为状态的“新奇”程度，由此设定内在奖励</strong>。</p><p>这么做的意义在与：对于与环境交互时经常访问到的状态，其预测网络产生的特征与随机网络产生的特征相差无几，但是如果突然出现一个新的状态或者很少遇见的状态，那么预测网络与随机网络输出之间的差异性比较大，由此说明状态的新颖性，并根据新颖性计算出内在奖励值。<strong>状态常见，则内在奖励小；状态罕见，则内在奖励大。</strong></p><p>这篇论文提出的RND方法在蒙特祖玛的复仇（Montezuma’s Revenge）上刷新了历史新高，并且没有使用专家示例经验（如模仿学习），也没有使用额外的游戏状态信息。</p><h1 id="方法"><a href="#方法" class="headerlink" title="方法"></a>方法</h1><p>奖励包括内在与外在两部分：</p><script type="math/tex; mode=display">r_{t}=e_{t}+i_{t}</script><p>其中，$e_t$为外部奖励，即环境给的，$i_t$为内部激励。</p><p>传统的内在奖励方法主要分为2类：</p><ol><li><p>计数</p><ol><li><p>在tabular setting中，状态、动作都是可数的，可以用一张表格来维护每个状态-动作对的访问次数，并且使用形如以下的公式来定义内在奖励：</p><script type="math/tex; mode=display">i_t=\frac{1}{n_t(s)}</script><script type="math/tex; mode=display">i_t=\frac{1}{\sqrt {n_t(s)}}</script></li><li><p>在非tabular setting中，状态、动作不完全可数，则可使用伪计数法（pseudo-counts），主要是使用状态密度估计的变化程度来衡量。参考论文：《Unifying count-based exploration and intrinsic motivation》</p></li></ol></li><li><p>预测，主要是通过前向(s,a-&gt;s’)/反向(s,s’-&gt;a)预测环境动态的误差来设计内在奖励</p></li></ol><blockquote><p>This paper introduces a different approach where the prediction problem is randomly generated. This involves two neural networks: a ﬁxed and randomly initialized target network which sets the prediction problem, and a predictor network trained on data collected by the agent.</p></blockquote><p>RND引入两个网络：</p><ol><li>预测网络，是强化学习网络的一部分，可训练，预测观测值的隐特征，$\hat{f}: \mathcal{O} \rightarrow \mathbb{R}^{k}$；</li><li>目标网络，额外引入的网络，不训练，固定参数，随机初始化，输出观测值的隐特征作为真值，$f: \mathcal{O} \rightarrow \mathbb{R}^{k}$。</li></ol><p>目标函数即为最小二乘误差：$|\hat{f}(\mathbf{x} ; \theta)-f(\mathbf{x})|^{2}$，优化参数$\theta_{\hat{f}}$。这个过程将随机初始化的神经网络蒸馏到一个学习的预测网络中去。</p><p><strong>上边的最小二乘误差也为内在奖励$i_t$</strong>，作者在使用这个内在奖励时还对其进行了归一化，因为任务不同，这个最小二乘的值偏好范围也不相同，所以作者对这个内在奖励进行了正态分布归一化的处理，主要是将最小二乘误差除以移动平均标准差估计。</p><p>作者说这个预测误差主要有以下几个影响因素：</p><ol><li>训练数据少。当样本中相似数据不多时，预测误差会大</li><li>环境的动态随机。环境的随机转换是前向预测误差增大的原因。</li><li>模型不适合。关键信息缺失（我也不明白是什么关键信息）或者预测的模型不足以拟合目标函数，会使得预测误差很大</li><li>动态学习。学习到最后不能稳定地拟合目标函数。</li></ol><p>作者还建议在使用内在奖励时，分开两个值函数估计，即一个值函数近似外部奖励，另一个值函数近似内部奖励。这是因为外部奖励稳定，内部奖励不稳定。</p><blockquote><p>Note that even where one is not trying to combine episodic and non-episodic reward streams, or reward streams with different discount factors, there may still be a beneﬁt to having separate value functions since there is an additional supervisory signal to the value function. This may be especially important for exploration bonuses since the extrinsic reward function is stationary whereas the intrinsic reward function is non-stationary.</p></blockquote><h2 id="状态归一化"><a href="#状态归一化" class="headerlink" title="状态归一化"></a>状态归一化</h2><p>作者在训练时还对状态进行了归一化处理，主要是以下流程：</p><ol><li>在训练开始之前与环境随机交互取得经验初始化移动平均均值和标准差</li><li>训练开始之后，状态减去移动平均均值</li><li>再除以移动平均标准差</li><li>将状态clip到[-5, 5]</li></ol><p>作者对状态归一化的原因主要是不归一化的状态其特征的方差非常低，携带信息少（属实不明白）：</p><blockquote><p>Observation normalization is often important in deep learning but it is crucial when using a random neural network as a target, since the parameters are frozen and hence cannot adjust to the scale of different datasets. </p><p>Lack of normalization can result in the variance of the embedding being extremely low and carrying little information about the inputs.</p></blockquote><p>这个状态归一化，作者用在随机目标网络和预测网络，但是没有用在策略网络中。</p><h1 id="伪代码"><a href="#伪代码" class="headerlink" title="伪代码"></a>伪代码</h1><p><img src="./random-network-distillation/pseudo.png" alt=""></p><h1 id="评价"><a href="#评价" class="headerlink" title="评价"></a>评价</h1><p>作者认为RND在局部探索中有用，在全局探索中作用不明显：</p><blockquote><p>We ﬁnd that the RND exploration bonus is sufﬁcient to deal with local exploration, i.e. exploring the consequences of short-term decisions, like whether to interact with a particular object, or avoid it. However global exploration that involves coordinated decisions over long time horizons is beyond the reach of our method.</p></blockquote>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;这篇论文提出了RND，是一种新奇的内在探索机制，主要原理是使预测网络的特征逼近随机网络的特征，以此使得预测网络对不熟悉的状态给出较大的预测误差，由此设定内在奖励。&lt;/p&gt;
&lt;p&gt;推荐程度中等：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;1个额外的固定、不训练的随机网络&lt;/li&gt;
&lt;li&gt;RND在强探索（hard exploration，奖励很稀疏）环境中表现不错&lt;/li&gt;
&lt;/ul&gt;
    
    </summary>
    
      <category term="ReinforcementLearning" scheme="http://StepNeverStop.github.io/categories/ReinforcementLearning/"/>
    
    
      <category term="rl" scheme="http://StepNeverStop.github.io/tags/rl/"/>
    
      <category term="exploration" scheme="http://StepNeverStop.github.io/tags/exploration/"/>
    
  </entry>
  
  <entry>
    <title>在DRL路上踩过的坑</title>
    <link href="http://StepNeverStop.github.io/rl-code-pit.html"/>
    <id>http://StepNeverStop.github.io/rl-code-pit.html</id>
    <published>2020-05-21T09:31:45.000Z</published>
    <updated>2020-05-21T09:38:42.273Z</updated>
    
    <content type="html"><![CDATA[<p>本博客用于记录在RL学习过程中踩过的坑点。</p><a id="more"></a><hr><ol><li><p>确定性策略梯度算法如DPG、DDPG、TD3等在优化Actor网络时是否引入动作噪声</p><p><strong>通过大量实验发现，在重新选择动作优化Actor网络时不需要引入噪声，直接使用确定性动作即可，引入噪声会影响算法的性能，甚至导致策略不收敛，这是当我在调试分层强化学习算法HIRO时发现的。</strong></p><p>之前的版本是：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> self.is_continuous:</span><br><span class="line">    mu = self.actor_net(feat)</span><br><span class="line">    pi = tf.clip_by_value(mu + self.action_noise(), <span class="number">-1</span>, <span class="number">1</span>)</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">    logits = self.actor_net(feat)</span><br><span class="line">    logp_all = tf.nn.log_softmax(logits)</span><br><span class="line">    gumbel_noise = tf.cast(self.gumbel_dist.sample([batch_size, self.a_counts]), dtype=tf.float32)</span><br><span class="line">    _pi = tf.nn.softmax((logp_all + gumbel_noise) / self.discrete_tau)</span><br><span class="line">    _pi_true_one_hot = tf.one_hot(tf.argmax(_pi, axis=<span class="number">-1</span>), self.a_counts)</span><br><span class="line">    _pi_diff = tf.stop_gradient(_pi_true_one_hot - _pi)</span><br><span class="line">    pi = _pi_diff + _pi</span><br><span class="line">q_actor = self.q_net(feat, pi)</span><br></pre></td></tr></table></figure><p>现在的版本是：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> self.is_continuous:</span><br><span class="line">    mu = self.actor_net(feat)</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">    logits = self.actor_net(feat)</span><br><span class="line">    _pi = tf.nn.softmax(logits)</span><br><span class="line">    _pi_true_one_hot = tf.one_hot(tf.argmax(logits, axis=<span class="number">-1</span>), self.a_counts, dtype=tf.float32)</span><br><span class="line">    _pi_diff = tf.stop_gradient(_pi_true_one_hot - _pi)</span><br><span class="line">    mu = _pi_diff + _pi</span><br><span class="line">q_actor = self.q_net(feat, mu)</span><br></pre></td></tr></table></figure></li></ol><p>2. </p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本博客用于记录在RL学习过程中踩过的坑点。&lt;/p&gt;
    
    </summary>
    
      <category term="ReinforcementLearning" scheme="http://StepNeverStop.github.io/categories/ReinforcementLearning/"/>
    
    
      <category term="RL" scheme="http://StepNeverStop.github.io/tags/RL/"/>
    
  </entry>
  
  <entry>
    <title>FeUdal Networks for Hierarchical Reinforcement Learning</title>
    <link href="http://StepNeverStop.github.io/FuNs.html"/>
    <id>http://StepNeverStop.github.io/FuNs.html</id>
    <published>2020-04-27T02:33:16.000Z</published>
    <updated>2020-04-27T04:46:44.286Z</updated>
    
    <content type="html"><![CDATA[<p><img src="./FuNs/illustration.png" alt=""></p><p>这篇论文提出了FuNs，将智能体决策分为两层——Manager产生子目标、Worker产生动作行为。两层均使用A2C方式进行优化，且梯度互不影响。</p><p>推荐程度中等：</p><ul><li>h-DQN式分层，PG式优化</li><li>隐藏状态空间设置子目标，不需要先验知识</li><li>上下两层策略均使用A2C的更新方式</li><li>应用于离散动作空间</li></ul><a id="more"></a><h1 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h1><p>论文地址：<a href="http://arxiv.org/abs/1703.01161" rel="external nofollow" target="_blank">http://arxiv.org/abs/1703.01161</a></p><p>pytorch复现代码：<a href="https://github.com/dnddnjs/feudal-montezuma" rel="external nofollow" target="_blank">https://github.com/dnddnjs/feudal-montezuma</a></p><p>这篇论文引入了FeUdal Networks(FuNs)，它是一个新奇的强化学习分层决策结构，它将决策模型分为Manager和Worker：</p><ul><li>Manager，在lower temporal resolution(低时间尺度)上做higher level决策，产生子目标。这个子目标是输入观察observation的隐状态空间上的方向向量，用于指定在$s_t$时刻之后$c$步应该朝着隐空间的什么方向移动；</li><li>Worker，在higher temporal resolution(更密级的时间尺度)上做lower level决策，产生执行的动作。</li></ul><h2 id="关注点"><a href="#关注点" class="headerlink" title="关注点"></a>关注点</h2><p>如何创建能够学习将其行为分解为有意义的原语，然后重用它们以更有效地获取新行为的智能体是一个长期存在的研究问题。</p><blockquote><p>How to create agents that can learn to decompose their behaviour into meaningful primitives and then reuse them to more efﬁciently acquire new behaviours is a long standing research question. The solution to this question may be an important stepping stone towards agents with general intelligence and competence.</p></blockquote><h2 id="主要贡献"><a href="#主要贡献" class="headerlink" title="主要贡献"></a>主要贡献</h2><p>这篇论文的创新点和特点主要有以下几个：</p><ol><li>提出一个一致的，端到端的，可微的模型，体现和概括了Feudal RL的原则</li><li>虽然上下两层都使用A2C的更新方式，但是上层策略的损失函数构造是根据自己提出的<strong>Transition Policy Gradients</strong>，利用了子目标的语义意义</li><li>提出了一个新奇的RNN结构，用在Manager模块中——<strong>Dilated LSTM</strong>，它增强了RNN的记忆能力，允许梯度在大的时间间隔内流动，允许在数百步长上进行有效的反向传播</li><li>上层控制器产生的子目标不再是显式的状态，而是隐状态空间上的方向向量</li></ol><h2 id="优点-效果"><a href="#优点-效果" class="headerlink" title="优点/效果"></a>优点/效果</h2><ul><li><p>FuNs大大提高了长期的信用分配和记忆。</p><blockquote><p>FuN signiﬁcantly improves long-term credit assignment and memorisation.</p></blockquote></li><li><p>鼓励与Manager设定的不同目标相关联的子策略的出现。</p><blockquote><p>encourages the emergence of sub-policies associated with different goals set by the Manager.</p></blockquote></li></ul><h1 id="文中精要"><a href="#文中精要" class="headerlink" title="文中精要"></a>文中精要</h1><p>下图为FuNs的整体框架示意图。</p><p><img src="./FuNs/illustration.png" alt=""></p><p>解析：</p><ul><li><p>上图中的灰色部分均为可学习、可微分的网络变量，在Manager框中即由Manager梯度更新，反之亦然。上图中$f^{\text {percept }}$既不属于Manager也不属于Worker，文中也没有提到它如何优化，我<strong>猜想</strong>它是通过Manager和Worker的Critic网络共享梯度更新优化的。</p></li><li><p>Manager使用Transition Policy Gradient进行优化，Worker使用Policy Gradient进行优化</p></li><li><p>$z_{t}=f^{\text {percept }}\left(x_{t}\right)$是特征提取层，CNN之类的，将观测值转换为长度为d的向量</p></li><li><p>$s_{t}=f^{M s p a c e}\left(z_{t}\right)$是Manager模块中的特征变换层，由几层全连接组成，从图上看维度不变，还是d</p></li><li><p>$h_{t}^{M}, \hat{g}_{t}=f^{M r n n}\left(s_{t}, h_{t-1}^{M}\right) ; g_{t}=\hat{g}_{t} /\left|\hat{g}_{t}\right|$是Manager模块的子目标输出层，子目标做了归一化操作，其中$f^{M r n n}$是论文中提出的Dilated LSTM</p></li><li><p>$w_{t}=\phi\left(\sum_{i=t-c}^{t} g_{i}\right)$是将子目标变换为一个可以与Worker动作表示矩阵相乘的向量，看形势上应该属于一个滑动窗口，窗口长度为$c$，在这个长度的一直做移动加和，每一个时间步都根据当前观测值输出子目标$g_i$，然后连续$c$个时间步的子目标加和之后通过$\phi$进行线性变换。</p><blockquote><p>A linear transform $\phi$ maps a goal $g_t$ into an embedding vector $w_t\in R^k$ , which is then combined via product with matrix $U_t$ (Workers output) to produce policy $\pi$.</p></blockquote></li><li><p>$h^{W}, U_{t}=f^{W r n n}\left(z_{t}, h_{t-1}^{W}\right)$是Worker的LSTM层，这里没有使用Dilated LSTM</p></li><li><p>$\pi_{t}=\operatorname{SoftMax}\left(U_{t} w_{t}\right)$是Worker的最终动作概率分布输出层，从文章看起来，FuNs只能应用于离散动作空间，因为其下层策略要产生的矩阵为$\mathrm{U}_{\mathrm{t}} \in \mathrm{R}^{|\mathrm{a}| \mathrm{xk}}$，即需要了解动作的数量。</p></li><li><p>Worker中的$k$为每个动作embedding向量的长度</p></li></ul><h2 id="Manager损失"><a href="#Manager损失" class="headerlink" title="Manager损失"></a>Manager损失</h2><p>Manager的损失函数，或者说是优化目标的梯度是这样的：</p><script type="math/tex; mode=display">\nabla g_{t}=A_{t}^{M} \nabla_{\theta} d_{\cos }\left(s_{t+c}-s_{t}, g_{t}(\theta)\right)</script><script type="math/tex; mode=display">A_{t}^{M}=R_{t}-V_{t}^{M}\left(x_{t}, \theta\right)</script><p>其中，$d_{\cos }(\alpha, \beta)=\alpha^{T} \beta /(|\alpha||\beta|)$是余弦相似度。与传统的PG损失不同，这里没有使用$log$操作，而且使用余弦相似度。由Critic网络的输入是$x_t$我猜想到上面结构图中的percept部分是是由Critic网络的梯度优化的。</p><p>注意，虽然$s_{t+c}$与$s_t$也是由Manager模块产生的，但是在优化中$s_{t+c}-s_{t}$并不传导梯度。</p><p>传统的PG目标函数梯度应该是这样的：</p><script type="math/tex; mode=display">\nabla_{\theta} \pi_{t}=\mathbb{E}\left[\left(R_{t}-V\left(s_{t}\right)\right) \nabla_{\theta} \log p\left(a_t | s_{t}\right)\right]</script><p>作者根据分层强化学习中上层策略产生不是动作，而是子目标，将上边式子通过分析、推理改写成下边这种形式：</p><p>$o_{t}=\mu\left(s_{t}, \theta\right)$选择子策略，$p\left(s_{t+c} | s_{t}, o_{t}\right)$表示在子策略条件下经过$c$步决策之后的隐状态分布，$\pi^{T P}\left(s_{t+c} | s_{t}\right)=p\left(s_{t+c} | s_{t}, \mu\left(s_{t}, \theta\right)\right)$描述给定起始状态的结束状态的分布，$s_{t+c}=\pi^{T P}\left(s_{t}\right)$是转移函数。</p><script type="math/tex; mode=display">\nabla_{\theta} \pi_{t}^{T P}=\mathbb{E}\left[\left(R_{t}-V\left(s_{t}\right)\right) \nabla_{\theta} \log p\left(s_{t+c} | s_{t}, \mu\left(s_{t}, \theta\right)\right)\right]</script><p>作者为了推导出$\nabla_{\theta} d_{\cos }\left(s_{t+c}-s_{t}, g_{t}(\theta)\right)$这种形式，假设转移模式是一种特殊的形式：$s_{t+c}-s_t$这个隐状态空间上的实际“运动”方向向量服从<strong><a href="https://en.wikipedia.org/wiki/Von_Mises%E2%80%93Fisher_distribution" rel="external nofollow" target="_blank">von Mises-Fisher</a></strong>分布，这个分布的均值即为上层策略产生的$g_t$，根据这个分布的性质，可以推导出如下公式：</p><script type="math/tex; mode=display">p\left(s_{t+c} | s_{t}, o_{t}\right) \propto e^{d_{\cos }\left(s_{t+c}-s_{t}, g_{t}\right)}</script><p>由此取对数可以推导出上边的梯度形式。</p><blockquote><p>A naive application of policy gradients requires the agent to learn from samples of these trajectories. But if we know where these trajectories are likely to end up, by modelling the transitions, then we can skip directly over the Worker’s behaviour and instead follow the policy gradient of the predicted transition. FuN assumes a particular form for the transition model: that the direction in state-space, s t+c −s t , follows a von Mises-Fisher distribution. Speciﬁcally, if the mean direction of the von Mises-Fisher distribution is given by g(o t ) (which for compactness we write as g t ) we would have p(s t+c | s t , o t ) ∝ e d cos (s t+c −s t ,g t ) . If this functional form were indeed correct, then we see that our proposed update heuristic for the Manager, eqn.7, is in fact the proper form for the transition policy gradient arrived at in eqn.10.</p></blockquote><h2 id="Worker损失"><a href="#Worker损失" class="headerlink" title="Worker损失"></a>Worker损失</h2><p>Worker的损失与传统的A2C一致：</p><script type="math/tex; mode=display">\nabla \pi_{t}=A_{t}^{D} \nabla_{\theta} \log \pi\left(a_{t} | x_{t} ; \theta\right)</script><script type="math/tex; mode=display">A_{t}^{D}=\left(R_{t}+\alpha R_{t}^{I}-V_{t}^{D}\left(x_{t} ; \theta\right)\right)</script><p>这里优势函数的target目标既包含外部奖励$R_t$，也包含内部奖励$R_t^I$，并用一个超参数$\alpha$来控制内部奖励的影响程度。注意，这里的Critic输入同样为$x_t$，所以我<strong>猜想</strong>特征表示部分由Manager和Worker各自的Critic共享梯度进行优化。</p><p>内部奖励是这么定义的：</p><script type="math/tex; mode=display">r_{t}^{I}=1 / c \sum_{i=1}^{c} d_{\cos }\left(s_{t}-s_{t-i}, g_{t-i}\right)</script><p>从这个公式可以看出，每一步的内部奖励需要往前计算$c$步余弦相似度，因此，如果设置的子目标持续步长$c$很大，那么将会引入额外的轨迹长度倍的计算开销。</p><p>作者使用方向向量，是因为与假设可以将智能体带到（可能）任意新的绝对位置相比，Worker能够更可靠地引起状态在隐空间上的方向转移。（真拗口）</p><blockquote><p>We use directions because it is more feasible for the Worker to be able to reliably cause directional shifts in the latent state than it is to assume that the Worker can take us to (potentially) arbitrary new absolute locations.</p><p>Note that the Worker’s intrinsic reward (eqn. 8) is based on the log-likelihood of state trajectory. Through that the FuN architecture actively encourages the functional form of the transition model to hold true. Because the Worker is learning to achieve the Manager’s direction, its transitions should, over time, closely follow a distribution around this direction, and hence our approximation for transition policy gradients should hold reasonably well.</p></blockquote><h2 id="Dilated-LSTM"><a href="#Dilated-LSTM" class="headerlink" title="Dilated LSTM"></a>Dilated LSTM</h2><p>说实话，这里没有完全理解。</p><p>公式是这样的：</p><script type="math/tex; mode=display">g_{t}=L S T M\left(s_{t}, \hat{h}_{t-1}^{t \% r} ; \theta^{L S T M}\right)</script><p>其中$\theta^{L S T M}$是共享的LSTM网络参数，cell_state是一个组，$h=\left\{\hat{h}^{i}\right\}_{i=1}^{r}$。$r$是一个dilation radius，也就是说这个LSTM网络包含许多个cell_state，也就是core。式子中的百分号<code>%</code>是做一个模的操作。然后作者说他们在实验中radius的设置与horizon相同，即$c=r$。</p><p><strong>我的猜想是这样的</strong>：</p><p>作者虽然想让上层策略在一个粗粒度的时间上进行决策，即产生子目标，但是下层策略的内部奖励依赖连续的子目标，也就是每一步的子目标。在这种情况下，上层策略必须每一步都产生子目标，那么既然产生了，不优化不就浪费了嘛，怎么利用呢？于是作者对每一时刻的子目标也做了平滑处理，即$w_{t}=\phi\left(\sum_{i=t-c}^{t} g_{i}\right)$。</p><p>问题在于作者想让上层策略的子目标可以持续$c$个时间步，$c$个时间步之后便失效，也就是说，对于上层策略，它的经验序列应该是这样的$s_t, s_{t+c}, s_{t+2c} … s_{t+nc}$，但是由于每一时间步都存在子目标，那么也存在这样的序列$s_{t+1}, s_{t+c+1}, s_{t+2c+1} … s_{t+nc+1}$。针对这种情况，其实有两种处理手段：</p><ol><li>将收集到的连续序列$s_t, s_{t+1}…$按时间步$c$切分成$c$条轨迹，然后分批次输入到LSTM中进行训练。其实相当于数据预处理过程</li><li>直接给LSTM输入收集到的连续序列$s_t, s_{t+1}…$，设置$r=c$组cell_state，也就是为$c$条轨迹设置$c$个不同的起始cell_state，由此来处理不切分的交叉时间尺度的经验序列，让梯度随着cell_state按不同轨迹自动传播。</li></ol><p>作者就是使用了第2种的处理方式。另外，作者在做对比实验的时候，给LSTM设置步长为40，给Dilated LSTM设置步长400，子目标持续步长$c=10$，也应该是这个道理，因为400/10=40，其实每个子目标序列的持续时间都是一直的，都是40个时间步。</p><p>从这个角度分析，我觉得作者说“它促进了更长时间的信用分配以及增强了RNN记忆能力”是一个噱头，因为其实是它只是在共享了LSTM的参数的基础上，对$c$条连续上层子目标序列<strong>分别</strong>做了LSTM运算，本质上没有将长时间的记忆融合进去。</p><h1 id="与h-DQN和OC的比较"><a href="#与h-DQN和OC的比较" class="headerlink" title="与h-DQN和OC的比较"></a>与h-DQN和OC的比较</h1><p>FuNs可以视为是h-DQN与OC的部分结合，但又不尽相同。</p><h2 id="h-DQN-vs-FuNs"><a href="#h-DQN-vs-FuNs" class="headerlink" title="h-DQN vs. FuNs"></a>h-DQN vs. FuNs</h2><p>相同：</p><ul><li>都是显式地将决策模型分为两层，上层决策子目标，下层决策具体动作</li><li>上层都是最大化外部环境累计期望回报</li><li>下层都使用了内在奖励</li><li>两者都将子目标作为下层策略的输入</li></ul><p>不同：</p><ul><li>FuNs上层产生的子目标是智能体的观测值在隐状态空间下的方向向量，而h-DQN上层产生的子目标是需要人为设计的，往往在原始观测状态空间上设置。</li><li>FuNs不需要先验知识</li><li>FuNs优化下层模型时，既使用了环境外部奖励，也使用了基于子目标产生的内部奖励，而h-DQN只使用了内部奖励</li><li>h-DQN是Q-Learning式的优化方式，即最小化均方误差，而FuNs是PG式的优化方式，即最大化累计期望奖励</li></ul><h2 id="OC-vs-FuNs"><a href="#OC-vs-FuNs" class="headerlink" title="OC vs. FuNs"></a>OC vs. FuNs</h2><p>相同：</p><ul><li>两者都是用了PG式的更新方式</li><li>两者都是端到端的</li></ul><p>不同：</p><ul><li>OC需要判断option的终止条件，FuNs固定上层策略的步长$c$</li><li>OC需要指定option的数量（离散），而FuNs产生的子目标为方向向量（连续）</li><li>OC需要为每个option构造一个下层策略模型，而FuNs共同一个下层策略模型</li><li>OC的下层策略输入不包含option，只是根据option选到下层策略，而FuNs的下层策略需要包含子目标输入</li><li>OC没有引入内在奖励，FuNs使用了内在奖励</li><li>OC可以应用于连续动作空间，而FuNs用于离散动作空间</li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;img src=&quot;./FuNs/illustration.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;p&gt;这篇论文提出了FuNs，将智能体决策分为两层——Manager产生子目标、Worker产生动作行为。两层均使用A2C方式进行优化，且梯度互不影响。&lt;/p&gt;
&lt;p&gt;推荐程度中等：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;h-DQN式分层，PG式优化&lt;/li&gt;
&lt;li&gt;隐藏状态空间设置子目标，不需要先验知识&lt;/li&gt;
&lt;li&gt;上下两层策略均使用A2C的更新方式&lt;/li&gt;
&lt;li&gt;应用于离散动作空间&lt;/li&gt;
&lt;/ul&gt;
    
    </summary>
    
      <category term="ReinforcementLearning" scheme="http://StepNeverStop.github.io/categories/ReinforcementLearning/"/>
    
    
      <category term="rl" scheme="http://StepNeverStop.github.io/tags/rl/"/>
    
      <category term="hrl" scheme="http://StepNeverStop.github.io/tags/hrl/"/>
    
  </entry>
  
  <entry>
    <title>记录在树莓派4B上的配置命令</title>
    <link href="http://StepNeverStop.github.io/raspverry4b.html"/>
    <id>http://StepNeverStop.github.io/raspverry4b.html</id>
    <published>2020-04-24T13:20:56.000Z</published>
    <updated>2020-04-25T02:23:11.276Z</updated>
    
    <content type="html"><![CDATA[<p>学习在树莓派4B上通过命令行配置各个模块。官方文档：<a href="https://www.raspberrypi.org/documentation/" rel="external nofollow" target="_blank">docs</a>。</p><a id="more"></a><p>打开配置窗口</p><p><code>sudo raspi-config</code></p><p>可以通过这个配置窗口打开摄像机模块</p><h1 id="换源"><a href="#换源" class="headerlink" title="换源"></a>换源</h1><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">sudo -s</span><br><span class="line">echo -e "deb http://mirrors.ustc.edu.cn/raspbian/raspbian/ stretch main contrib non-free rpi \n deb-src http://mirrors.ustc.edu.cn/raspbian/raspbian/ stretch main contrib non-free rpi" &gt; /etc/apt/sources.list</span><br><span class="line">exit</span><br><span class="line">sudo apt update &amp;&amp; sudo apt -y upgrade</span><br></pre></td></tr></table></figure><h1 id="更新库"><a href="#更新库" class="headerlink" title="更新库"></a>更新库</h1><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">sudo apt-get update</span><br><span class="line">sudo apt-get upgrade</span><br><span class="line">sudo apt full-upgrade</span><br></pre></td></tr></table></figure><h1 id="安装-BerryConda"><a href="#安装-BerryConda" class="headerlink" title="安装 BerryConda"></a>安装 BerryConda</h1><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">wget https://github.com/jjhelmus/berryconda/releases/download/v2.0.0/Berryconda3-2.0.0-Linux-armv7l.sh</span><br><span class="line">chmod a+x Berryconda3-2.0.0-Linux-armv7l.sh</span><br><span class="line">./Berryconda3-2.0.0-Linux-armv7l.sh</span><br></pre></td></tr></table></figure><p>一路<code>ENTER</code>+<code>yes</code>就ok了。</p><p><code>source ~/.bashrc</code></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">conda config --add channels https://mirrors.ustc.edu.cn/anaconda/pkgs/free/</span><br><span class="line">conda config --add channels https://mirrors.ustc.edu.cn/anaconda/pkgs/main/</span><br><span class="line">conda config --set show_channel_urls yes</span><br><span class="line">conda config --show</span><br></pre></td></tr></table></figure><h1 id="使用摄像机模块拍照"><a href="#使用摄像机模块拍照" class="headerlink" title="使用摄像机模块拍照"></a>使用摄像机模块拍照</h1><p>在终端使用命令<code>raspistill</code>，点击<a href="https://www.raspberrypi.org/documentation/usage/camera/raspicam/raspistill.md" rel="external nofollow" target="_blank">这里</a>查看详细API。</p><p><code>raspistill -o 路径/image.jpg -w 640 -h 480</code></p><p><code>raspistill -vf -hf -o cam2.jpg</code>这个可以指定水平和垂直翻转图像</p><p>录像的命令是<code>raspivid</code>，点击<a href="https://www.raspberrypi.org/documentation/usage/camera/raspicam/raspivid.md" rel="external nofollow" target="_blank">这里</a>查看详细API，比如</p><p><code>raspivid -o 路径/video.h264</code></p><p>如要录制<code>.mp4</code>格式，需要先安装MP4BOX：</p><p><code>sudo apt install -y gpac</code></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> Capture 30 seconds of raw video at 640x480 and 150kB/s bit rate into a pivideo.h264 file:</span></span><br><span class="line">raspivid -t 30000 -w 640 -h 480 -fps 25 -b 1200000 -p 0,0,640,480 -o pivideo.h264</span><br><span class="line"><span class="meta">#</span><span class="bash"> Wrap the raw video with an MP4 container:</span></span><br><span class="line">MP4Box -add pivideo.h264 pivideo.mp4</span><br><span class="line"><span class="meta">#</span><span class="bash"> Remove the <span class="built_in">source</span> raw file, leaving the remaining pivideo.mp4 file to play</span></span><br><span class="line">rm pivideo.h264</span><br></pre></td></tr></table></figure><h2 id="使用python拍照"><a href="#使用python拍照" class="headerlink" title="使用python拍照"></a>使用python拍照</h2><p>创建一个<code>.py</code>文件。</p><p>安装<code>picamera</code>：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">pip install picamera</span><br><span class="line"><span class="meta">#</span><span class="bash"> If you wish to use the classes <span class="keyword">in</span> the picamera.array module <span class="keyword">then</span> specify the “array” option <span class="built_in">which</span> will pull <span class="keyword">in</span> numpy as a dependency:</span></span><br><span class="line">pip install "picamera[array]"</span><br><span class="line">sudo pip install -U picamera # 更新</span><br></pre></td></tr></table></figure><p>下面是我写的一个测试各项功能的脚本：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">from</span> time <span class="keyword">import</span> sleep</span><br><span class="line"><span class="keyword">from</span> picamera <span class="keyword">import</span> PiCamera, Color</span><br><span class="line"></span><br><span class="line">BASE_FOLD = <span class="string">'./'</span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Cam</span><span class="params">(object)</span>:</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, camera, path=<span class="string">'./'</span>, alpha=<span class="number">0</span>)</span>:</span></span><br><span class="line">        <span class="string">'''</span></span><br><span class="line"><span class="string">        params:</span></span><br><span class="line"><span class="string">            camera: PiCamera</span></span><br><span class="line"><span class="string">            alpha: the degree of transparency, [0, 255]</span></span><br><span class="line"><span class="string">        '''</span></span><br><span class="line">        self.camera = camera</span><br><span class="line">        self.path = path</span><br><span class="line">        self.create_date_folder()</span><br><span class="line">        self.alpha = alpha</span><br><span class="line">        self.start()</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">create_date_folder</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="string">'''</span></span><br><span class="line"><span class="string">        创建存放图片和视频的文件夹</span></span><br><span class="line"><span class="string">        '''</span></span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> os.path.exists(self.path+<span class="string">'images'</span>):</span><br><span class="line">            os.makedirs(self.path+<span class="string">'images'</span>)</span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> os.path.exists(self.path+<span class="string">'vedios'</span>):</span><br><span class="line">            os.makedirs(self.path+<span class="string">'vedios'</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">start</span><span class="params">(self, alpha=<span class="number">-1</span>)</span>:</span></span><br><span class="line">        <span class="keyword">if</span> alpha&lt;<span class="number">0</span>:</span><br><span class="line">            alpha = self.alpha</span><br><span class="line">        self.camera.start_preview(alpha=alpha)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">close</span><span class="params">(self)</span>:</span></span><br><span class="line">        self.camera.stop_preview()</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">review</span><span class="params">(self, time)</span>:</span></span><br><span class="line">        <span class="string">'''</span></span><br><span class="line"><span class="string">        检视</span></span><br><span class="line"><span class="string">        params:</span></span><br><span class="line"><span class="string">            time: sleep time</span></span><br><span class="line"><span class="string">            rot: rotation angle, within [0, 90, 180, 270, 360]</span></span><br><span class="line"><span class="string">        '''</span></span><br><span class="line">        sleep(time)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">capture</span><span class="params">(self, time, name)</span>:</span></span><br><span class="line">        <span class="string">'''</span></span><br><span class="line"><span class="string">        拍摄单张图片</span></span><br><span class="line"><span class="string">        '''</span></span><br><span class="line">        sleep(time)</span><br><span class="line">        self.camera.capture(self.path+<span class="string">f'images/<span class="subst">&#123;name&#125;</span>.jpg'</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">capture_batch</span><span class="params">(self, time, name, n)</span>:</span></span><br><span class="line">        <span class="string">'''</span></span><br><span class="line"><span class="string">        连续拍照</span></span><br><span class="line"><span class="string">        '''</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(n):</span><br><span class="line">            self.capture(time, name=name+<span class="string">f'-<span class="subst">&#123;i&#125;</span>'</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">record</span><span class="params">(self, time, name)</span>:</span></span><br><span class="line">        <span class="string">'''</span></span><br><span class="line"><span class="string">        录制视频</span></span><br><span class="line"><span class="string">        '''</span></span><br><span class="line">        self.camera.start_recording(self.path+<span class="string">f'vedios/<span class="subst">&#123;name&#125;</span>.h264'</span>)</span><br><span class="line">        sleep(time)</span><br><span class="line">        self.camera.stop_recording()</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">set_camera_attrs</span><span class="params">(self, d)</span>:</span></span><br><span class="line">        <span class="string">'''</span></span><br><span class="line"><span class="string">        批量设置相机属性</span></span><br><span class="line"><span class="string">        '''</span></span><br><span class="line">        <span class="comment"># self.close()</span></span><br><span class="line">        <span class="keyword">for</span> k, v <span class="keyword">in</span> d.items():</span><br><span class="line">            self.set_camera_attr(k, v)</span><br><span class="line">        <span class="comment"># self.start()</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">set_camera_attr</span><span class="params">(self, k, v)</span>:</span></span><br><span class="line">        <span class="string">'''</span></span><br><span class="line"><span class="string">        设置相机某个属性</span></span><br><span class="line"><span class="string">        '''</span></span><br><span class="line">        setattr(self.camera, k, v)</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">traverse_effects</span><span class="params">(self, time, name)</span>:</span></span><br><span class="line">        <span class="string">'''</span></span><br><span class="line"><span class="string">        遍历图像特效</span></span><br><span class="line"><span class="string">        '''</span></span><br><span class="line">        <span class="keyword">for</span> effect <span class="keyword">in</span> self.camera.IMAGE_EFFECTS:</span><br><span class="line">            self.camera.image_effect = effect</span><br><span class="line">            self.camera.annotate_text = <span class="string">"Effect: %s"</span> % effect</span><br><span class="line">            self.capture(time, name+<span class="string">'-'</span>+str(effect))</span><br><span class="line">        self.camera.image_effect = <span class="string">'none'</span></span><br><span class="line">        self.camera.annotate_text = <span class="string">''</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">traverse_exposure_mode</span><span class="params">(self, time, name)</span>:</span></span><br><span class="line">        <span class="string">'''</span></span><br><span class="line"><span class="string">        遍历曝光模式</span></span><br><span class="line"><span class="string">        '''</span></span><br><span class="line">        <span class="keyword">for</span> exposure_mode <span class="keyword">in</span> self.camera.EXPOSURE_MODES:</span><br><span class="line">            self.camera.exposure_mode = exposure_mode</span><br><span class="line">            self.camera.annotate_text = <span class="string">"Exposure Mode: %s"</span> % exposure_mode</span><br><span class="line">            self.capture(time, name+<span class="string">'-'</span>+str(exposure_mode))</span><br><span class="line">        self.camera.exposure_mode = <span class="string">'auto'</span></span><br><span class="line">        self.camera.annotate_text = <span class="string">''</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">traverse_white_balance</span><span class="params">(self, time, name)</span>:</span></span><br><span class="line">        <span class="string">'''</span></span><br><span class="line"><span class="string">        遍历白平衡模式</span></span><br><span class="line"><span class="string">        '''</span></span><br><span class="line">        <span class="keyword">for</span> awb_mode <span class="keyword">in</span> self.camera.AWB_MODES:</span><br><span class="line">            self.camera.awb_mode = awb_mode</span><br><span class="line">            self.camera.annotate_text = <span class="string">"Write Balamce: %s"</span> % awb_mode</span><br><span class="line">            self.capture(time, name+<span class="string">'-'</span>+str(awb_mode))</span><br><span class="line">        self.camera.awb_mode = <span class="string">'auto'</span></span><br><span class="line">        self.camera.annotate_text = <span class="string">''</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">run</span><span class="params">()</span>:</span></span><br><span class="line">    camera_config=&#123;</span><br><span class="line">        <span class="string">'rotation'</span>: <span class="number">180</span>,    <span class="comment"># rotation angle, within [0, 90, 180, 270, 360]</span></span><br><span class="line">        <span class="string">'resolution'</span>: (<span class="number">800</span>, <span class="number">600</span>), <span class="comment"># The maximum resolution is 2592×1944 for still photos, and 1920×1080 for video recording. The minimum resolution is 64×64.</span></span><br><span class="line">        <span class="string">'framerate'</span>: <span class="number">15</span>,</span><br><span class="line">        <span class="string">'annotate_text'</span>: <span class="string">"Hello world!"</span>, </span><br><span class="line">        <span class="string">'annotate_text_size'</span>: <span class="number">50</span>,   <span class="comment">#[6, 160], default 32</span></span><br><span class="line">        <span class="string">'annotate_background'</span>: Color(<span class="string">'blue'</span>),</span><br><span class="line">        <span class="string">'annotate_foreground'</span>: Color(<span class="string">'red'</span>),</span><br><span class="line">        <span class="string">'brightness'</span>: <span class="number">50</span>,   <span class="comment"># 亮度  [0, 100], default 50</span></span><br><span class="line">        <span class="string">'contrast'</span>: <span class="number">0</span>   <span class="comment"># 对比度</span></span><br><span class="line">    &#125;</span><br><span class="line">    camera = Cam(PiCamera(), path=BASE_FOLD)</span><br><span class="line">    camera.set_camera_attrs(camera_config)</span><br><span class="line">    camera.review(<span class="number">2</span>)</span><br><span class="line">    camera.capture(<span class="number">2</span>, <span class="string">'test_capture'</span>)</span><br><span class="line">    camera.capture_batch(<span class="number">2</span>, <span class="string">'test_capture_batch'</span>, <span class="number">4</span>)</span><br><span class="line">    camera.record(<span class="number">2</span>, <span class="string">'test_record'</span>)</span><br><span class="line">    camera.traverse_effects(<span class="number">2</span>, <span class="string">'test_traverse_effects'</span>)</span><br><span class="line">    camera.traverse_exposure_mode(<span class="number">2</span>, <span class="string">'test_traverse_exposure_mode'</span>)</span><br><span class="line">    camera.traverse_white_balance(<span class="number">2</span>, <span class="string">'test_traverse_white_balance'</span>)</span><br><span class="line">    camera.close()</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">"__main__"</span>:</span><br><span class="line">    run()</span><br></pre></td></tr></table></figure><p>更多复杂操作，请看官方<a href="https://picamera.readthedocs.io/en/release-1.13/recipes1.html" rel="external nofollow" target="_blank">API文档</a>。</p><h1 id="GPIO"><a href="#GPIO" class="headerlink" title="GPIO"></a>GPIO</h1><p>这个库好像是用来与树莓派的引脚进行交互的。</p><h1 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h1><ol><li><a href="https://blog.csdn.net/xuzhexing/article/details/99404943" rel="external nofollow" target="_blank">树莓派 更换国内源，安装vim，berryconda，opencv</a></li></ol>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;学习在树莓派4B上通过命令行配置各个模块。官方文档：&lt;a href=&quot;https://www.raspberrypi.org/documentation/&quot; rel=&quot;external nofollow&quot; target=&quot;_blank&quot;&gt;docs&lt;/a&gt;。&lt;/p&gt;
    
    </summary>
    
      <category term="Raspberry" scheme="http://StepNeverStop.github.io/categories/Raspberry/"/>
    
    
      <category term="os" scheme="http://StepNeverStop.github.io/tags/os/"/>
    
      <category term="raspberry" scheme="http://StepNeverStop.github.io/tags/raspberry/"/>
    
  </entry>
  
  <entry>
    <title>在MacOS上为树莓派烧录系统</title>
    <link href="http://StepNeverStop.github.io/burn-system2raspberry-in-macos.html"/>
    <id>http://StepNeverStop.github.io/burn-system2raspberry-in-macos.html</id>
    <published>2020-04-24T08:03:37.000Z</published>
    <updated>2020-04-24T10:00:23.390Z</updated>
    
    <content type="html"><![CDATA[<p>此篇博文用于记录在MacOS系统上为TF卡烧录树莓派操作系统。</p><a id="more"></a><h1 id="下载镜像"><a href="#下载镜像" class="headerlink" title="下载镜像"></a>下载镜像</h1><p>网址：<a href="https://www.raspberrypi.org/downloads/" rel="external nofollow" target="_blank">https://www.raspberrypi.org/downloads/</a></p><p><img src="./burn-system2raspberry-in-macos/downloads.png" alt=""></p><p>我试验下来，上图中的<code>Raspberry Pi Imager for macOS</code>并不好用。因此，我们点左下角的Raspbian图片下载镜像。</p><p><img src="./burn-system2raspberry-in-macos/imgs.png" alt=""></p><p>这边目前有三种版本：</p><ol><li>系统+桌面+推荐软件</li><li>系统+桌面</li><li>系统</li></ol><p>我选择了最简洁的Raspbian Buster Lite，下载<code>.zip</code>并解压出其中的<code>.img</code>文件。</p><h1 id="TF卡格式化"><a href="#TF卡格式化" class="headerlink" title="TF卡格式化"></a>TF卡格式化</h1><p>首先需要下载格式化工具，我这边使用的是<code>SD Memory Card Formatter</code>，这个软件在<code>windows</code>和<code>macos</code>上都可以用。这里是<code>macos</code>下的下载链接：<a href="https://www.sdcard.org/downloads/formatter/eula_mac/index.html" rel="external nofollow" target="_blank">https://www.sdcard.org/downloads/formatter/eula_mac/index.html</a></p><p><img src="./burn-system2raspberry-in-macos/formatter.png" alt=""></p><h2 id="格式化"><a href="#格式化" class="headerlink" title="格式化"></a>格式化</h2><ol><li>插入<code>TF</code>卡</li><li>打开<code>SD Memory Card Formatter</code></li></ol><p><img src="./burn-system2raspberry-in-macos/start.png" alt=""></p><p>注意不要格式化错了卡，假如你插入了多个TF卡。上图中的<code>Volume label</code>是格式化后磁盘的命名。</p><p><img src="./burn-system2raspberry-in-macos/end.png" alt=""></p><h1 id="开始烧录"><a href="#开始烧录" class="headerlink" title="开始烧录"></a>开始烧录</h1><h2 id="查看驱动器列表"><a href="#查看驱动器列表" class="headerlink" title="查看驱动器列表"></a>查看驱动器列表</h2><p>在控制台输入命令：<code>diskutil list</code></p><p><img src="./burn-system2raspberry-in-macos/cmd1.png" alt=""></p><p>这里，我们获取到TF卡的磁盘路径为<code>/dev/disk2</code></p><h2 id="取消TF卡的挂载"><a href="#取消TF卡的挂载" class="headerlink" title="取消TF卡的挂载"></a>取消TF卡的挂载</h2><p>在控制台输入命令：<code>diskutil unmountDisk + SD卡设备路径</code></p><p><img src="./burn-system2raspberry-in-macos/cmd2.png" alt=""></p><h2 id="开始烧录-1"><a href="#开始烧录-1" class="headerlink" title="开始烧录"></a>开始烧录</h2><p>在控制台输入命令：<code>sudo dd if=镜像路径 of=SD卡设备的路径 bs=1m;sync</code>，并输入管理员密码。</p><p>注意：文件路径不要出现中文。可以将<code>bs=1m</code>改为<code>bs=4m</code>加快烧录的速度。</p><p>这个时间有点长，需要耐心等待，400M的镜像大概耗时2分钟左右。</p><p><img src="./burn-system2raspberry-in-macos/success.png" alt=""></p><h2 id="编写树莓派的ssh配置与wifi配置文件"><a href="#编写树莓派的ssh配置与wifi配置文件" class="headerlink" title="编写树莓派的ssh配置与wifi配置文件"></a>编写树莓派的ssh配置与wifi配置文件</h2><p>新建两个<code>.txt</code>文件，分别命名为：</p><ul><li>ssh</li><li>wpa_supplicant.conf</li></ul><p>注意，取消<code>.txt</code>后缀。</p><p>ssh文件为空即可，wpa_supplicant.conf文件中写入：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">country=CN</span><br><span class="line">ctrl_interface=DIR=/var/run/wpa_supplicant Group=netdev</span><br><span class="line">update_config=1</span><br><span class="line"></span><br><span class="line">network=&#123;</span><br><span class="line">ssid=&quot;Wifi1的名字&quot;</span><br><span class="line">psk=&quot;密码&quot;</span><br><span class="line">priority=优先级，越大越优先</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">network=&#123;</span><br><span class="line">ssid=&quot;Wifi2的名字&quot;</span><br><span class="line">psk=&quot;密码&quot;</span><br><span class="line">priority=优先级</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><strong>然后将这两个文件移动至烧录系统镜像后TF卡的根目录。</strong></p><p>这里有两个坑点：</p><ol><li>SSID名字中不能有符号<code>.</code></li><li>优先级范围为1-10</li></ol><h2 id="推出TF卡"><a href="#推出TF卡" class="headerlink" title="推出TF卡"></a>推出TF卡</h2><p><code>diskutil eject SD卡设备路径</code></p><h1 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h1><ol><li><a href="https://www.jianshu.com/p/e95c406badaa" rel="external nofollow" target="_blank">MacOS下树莓派烧录img/iso文件到SD卡</a></li><li><a href="https://www.jianshu.com/p/4c5b7c1ad2a3" rel="external nofollow" target="_blank">mac下烧写树莓派系统</a></li><li><a href="http://relyn.cn/share/38" rel="external nofollow" target="_blank">树莓派raspbian系统自动连接WIFI开启ssh</a></li></ol>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;此篇博文用于记录在MacOS系统上为TF卡烧录树莓派操作系统。&lt;/p&gt;
    
    </summary>
    
      <category term="MacOS" scheme="http://StepNeverStop.github.io/categories/MacOS/"/>
    
    
      <category term="os" scheme="http://StepNeverStop.github.io/tags/os/"/>
    
      <category term="raspberry" scheme="http://StepNeverStop.github.io/tags/raspberry/"/>
    
  </entry>
  
  <entry>
    <title>LeetCode刷题记录(python)</title>
    <link href="http://StepNeverStop.github.io/leetcode.html"/>
    <id>http://StepNeverStop.github.io/leetcode.html</id>
    <published>2020-04-24T03:58:00.000Z</published>
    <updated>2020-04-24T06:20:18.327Z</updated>
    
    <content type="html"><![CDATA[<p>这篇博客用于学习并记录在LeetCode刷题的过程及其相关题目解决思路。</p><a id="more"></a><h1 id="栈"><a href="#栈" class="headerlink" title="栈"></a>栈</h1><h2 id="20-有效的括号"><a href="#20-有效的括号" class="headerlink" title="20. 有效的括号"></a>20. <a href="https://leetcode-cn.com/problems/valid-parentheses/" rel="external nofollow" target="_blank">有效的括号</a></h2><p>判断字符串内括号顺序是否正确，<code>()[]</code>正确，<code>([)]</code>不正确，因为左方括号与右圆括号不合法。</p><p>解题思路1：</p><p>将左括号进栈，遍历到右括号就出栈，判断最后栈中是否还有剩余。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">isValid</span><span class="params">(self, s: str)</span> -&gt; bool:</span></span><br><span class="line">        <span class="keyword">if</span> len(s) % <span class="number">2</span> == <span class="number">1</span>:</span><br><span class="line">            <span class="keyword">return</span> <span class="keyword">False</span></span><br><span class="line">        d = &#123;<span class="string">')'</span>:<span class="string">'('</span>, <span class="string">']'</span>:<span class="string">'['</span>, <span class="string">'&#125;'</span>:<span class="string">'&#123;'</span>&#125;</span><br><span class="line">        tmp = [<span class="string">'t'</span>]<span class="comment"># 终结符，为了避免tmp为空时执行.pop方法出错</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> s:</span><br><span class="line">            <span class="keyword">if</span> i <span class="keyword">in</span> d.keys():</span><br><span class="line">                <span class="keyword">if</span> d.get(i) != tmp.pop():</span><br><span class="line">                    <span class="keyword">return</span> <span class="keyword">False</span></span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                tmp.append(i)</span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">False</span> <span class="keyword">if</span> len(tmp) != <span class="number">1</span> <span class="keyword">else</span> <span class="keyword">True</span></span><br></pre></td></tr></table></figure><p>解题思路2：</p><p>将<code>()</code>,<code>[]</code>,<code>{}</code>替换为空，如果最终字符串为空，则为真。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">isValid</span><span class="params">(self, s: str)</span> -&gt; bool:</span></span><br><span class="line">        <span class="keyword">if</span> len(s)%<span class="number">2</span>:</span><br><span class="line">            <span class="keyword">return</span> <span class="keyword">False</span></span><br><span class="line">        <span class="keyword">while</span> <span class="string">'()'</span> <span class="keyword">in</span> s <span class="keyword">or</span> <span class="string">'[]'</span> <span class="keyword">in</span> s <span class="keyword">or</span> <span class="string">'&#123;&#125;'</span> <span class="keyword">in</span> s:</span><br><span class="line">            s = s.replace(<span class="string">'[]'</span>,<span class="string">''</span>).replace(<span class="string">'()'</span>,<span class="string">''</span>).replace(<span class="string">'&#123;&#125;'</span>,<span class="string">''</span>)</span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">False</span> <span class="keyword">if</span> len(s) <span class="keyword">else</span> <span class="keyword">True</span></span><br></pre></td></tr></table></figure><h2 id="42-接雨水"><a href="#42-接雨水" class="headerlink" title="42. 接雨水"></a>42. <a href="https://leetcode-cn.com/problems/trapping-rain-water/" rel="external nofollow" target="_blank">接雨水</a></h2><p>这道题挺有意思的，根据数组判断某个地形可容纳的积水。</p><p><img src="./leetcode/42.png" alt=""></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">输入: [0,1,0,2,1,0,1,3,2,1,2,1]</span><br><span class="line">输出: 6</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">trap</span><span class="params">(self, height: List[int])</span> -&gt; int:</span></span><br><span class="line">        <span class="string">'''</span></span><br><span class="line"><span class="string">        思路：</span></span><br><span class="line"><span class="string">            1. 先找到左侧制高点，将数组分为两部分，右边部分reversed一下</span></span><br><span class="line"><span class="string">            2. 在两部分数组都执行“左低右高”式累加即可</span></span><br><span class="line"><span class="string">        '''</span></span><br><span class="line">        <span class="keyword">if</span> len(height) &lt; <span class="number">3</span>: <span class="comment"># 数组至少有三个元素才能积水</span></span><br><span class="line">            <span class="keyword">return</span> <span class="number">0</span></span><br><span class="line">        max_idx = height.index(max(height))</span><br><span class="line">        h1 = height[:max_idx+<span class="number">1</span>]</span><br><span class="line">        h2 = list(reversed(height[max_idx:]))</span><br><span class="line">        <span class="function"><span class="keyword">def</span> <span class="title">get_v</span><span class="params">(arr)</span>:</span></span><br><span class="line">            v = <span class="number">0</span></span><br><span class="line">            left = arr[<span class="number">0</span>]</span><br><span class="line">            cv = <span class="number">0</span>  <span class="comment"># 某个区间的累计蓄水</span></span><br><span class="line">            <span class="keyword">for</span> h <span class="keyword">in</span> arr:</span><br><span class="line">                <span class="keyword">if</span> h &lt; left:</span><br><span class="line">                    cv += left - h</span><br><span class="line">                <span class="keyword">else</span>:</span><br><span class="line">                    v += cv</span><br><span class="line">                    cv = <span class="number">0</span></span><br><span class="line">                    left = h</span><br><span class="line">            <span class="keyword">return</span> v</span><br><span class="line">        <span class="keyword">return</span> get_v(h1) + get_v(h2)</span><br></pre></td></tr></table></figure><h2 id="71-简化路径"><a href="#71-简化路径" class="headerlink" title="71. 简化路径"></a>71. <a href="https://leetcode-cn.com/problems/simplify-path/" rel="external nofollow" target="_blank">简化路径</a></h2><p>以 Unix 风格给出一个文件的<strong>绝对路径</strong>。右侧不带斜杠<code>/</code>，去除相对路径符号<code>.</code>（当前目录），<code>..</code>（上级目录）。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">simplifyPath</span><span class="params">(self, path: str)</span> -&gt; str:</span></span><br><span class="line">        <span class="string">'''</span></span><br><span class="line"><span class="string">        思路：</span></span><br><span class="line"><span class="string">            1. 按'/'切分</span></span><br><span class="line"><span class="string">            2. 去除空字符串</span></span><br><span class="line"><span class="string">            3. 去除当前目录符号'.'</span></span><br><span class="line"><span class="string">            4. 抵消'..'前的目录</span></span><br><span class="line"><span class="string">            5. 给最后输入加入左斜杠'/'</span></span><br><span class="line"><span class="string">        '''</span></span><br><span class="line">        dirs = path.split(<span class="string">'/'</span>)</span><br><span class="line">        <span class="keyword">while</span> <span class="string">''</span> <span class="keyword">in</span> dirs:</span><br><span class="line">            dirs.pop(dirs.index(<span class="string">''</span>))</span><br><span class="line">        <span class="keyword">while</span> <span class="string">'.'</span> <span class="keyword">in</span> dirs:</span><br><span class="line">            dirs.pop(dirs.index(<span class="string">'.'</span>))</span><br><span class="line">        <span class="keyword">while</span> <span class="string">'..'</span> <span class="keyword">in</span> dirs:</span><br><span class="line">            idx = dirs.index(<span class="string">'..'</span>)</span><br><span class="line">            dirs.pop(idx)</span><br><span class="line">            <span class="keyword">if</span> idx &gt; <span class="number">0</span>:</span><br><span class="line">                dirs.pop(idx<span class="number">-1</span>)</span><br><span class="line">        <span class="keyword">return</span> <span class="string">'/'</span> + <span class="string">'/'</span>.join(dirs)</span><br></pre></td></tr></table></figure><h1 id="队列"><a href="#队列" class="headerlink" title="队列"></a>队列</h1><p>…</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;这篇博客用于学习并记录在LeetCode刷题的过程及其相关题目解决思路。&lt;/p&gt;
    
    </summary>
    
      <category term="Python" scheme="http://StepNeverStop.github.io/categories/Python/"/>
    
    
      <category term="python" scheme="http://StepNeverStop.github.io/tags/python/"/>
    
      <category term="leetcode" scheme="http://StepNeverStop.github.io/tags/leetcode/"/>
    
  </entry>
  
  <entry>
    <title>配置OverLeaf私人服务器</title>
    <link href="http://StepNeverStop.github.io/overleaf.html"/>
    <id>http://StepNeverStop.github.io/overleaf.html</id>
    <published>2020-04-16T14:53:53.000Z</published>
    <updated>2020-04-16T14:57:57.678Z</updated>
    
    <content type="html"><![CDATA[<p>此文转载自好友<a href="[https://bluefisher.github.io/2020/04/16/Overleaf-%E6%9C%8D%E5%8A%A1%E5%99%A8%E9%85%8D%E7%BD%AE/](https://bluefisher.github.io/2020/04/16/Overleaf-服务器配置/">BlueFisher</a>。</p><a id="more"></a><ol><li><p>首先根据官方教程 <a href="https://docs.docker.com/engine/install/ubuntu/" rel="external nofollow" target="_blank">https://docs.docker.com/engine/install/ubuntu/</a> 确保服务器已经安装了 Docker，同时根据 <a href="https://docs.docker.com/compose/install/" rel="external nofollow" target="_blank">https://docs.docker.com/compose/install/</a> 安装 Docker Compose。</p></li><li><p>拉取最新的 overleaf 服务器版本</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ docker pull sharelatex/sharelatex</span><br></pre></td></tr></table></figure></li><li><p>在用户目录 <code>~</code> 下新建文件夹 <code>~/sharelatex/, ~/sharelatex/sharelatex_data/, ~/sharelatex/mongo_data/, ~/sharelatex/redis_data/</code></p></li><li><p>下载 <a href="https://github.com/overleaf/overleaf/blob/master/docker-compose.yml" rel="external nofollow" target="_blank">docker-compose.yml</a> 文件，并存在 <code>~/sharelatex/</code> 文件夹中</p></li><li><p>根据需要修改 docker-compose.yml 文件，可以更改服务器映射的端口号 <code>ports</code> ，修改 sharelatex, mongo 和 redis 的<code>volumes</code> 到步骤3创建的文件夹中</p></li><li><p>进入 <code>~sharelatex</code> 启动 docker-compose.yml</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> docker-compose up</span></span><br></pre></td></tr></table></figure></li><li><p>由于默认安装的是最小版本 TeXLive，如果要安装完整包，执行</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> docker <span class="built_in">exec</span> sharelatex tlmgr install scheme-full</span></span><br></pre></td></tr></table></figure><p>或者也可以安装任意的单个包，只需要把 <code>sheme-full</code> 替换为包的名称即可</p></li><li><p>第一次启动镜像后，访问 <code>/launchpad</code> 页面设置管理员账号</p></li></ol>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;此文转载自好友&lt;a href=&quot;[https://bluefisher.github.io/2020/04/16/Overleaf-%E6%9C%8D%E5%8A%A1%E5%99%A8%E9%85%8D%E7%BD%AE/](https://bluefisher.github.io/2020/04/16/Overleaf-服务器配置/&quot;&gt;BlueFisher&lt;/a&gt;。&lt;/p&gt;
    
    </summary>
    
      <category term="小知识" scheme="http://StepNeverStop.github.io/categories/%E5%B0%8F%E7%9F%A5%E8%AF%86/"/>
    
    
      <category term="docker" scheme="http://StepNeverStop.github.io/tags/docker/"/>
    
      <category term="overleaf" scheme="http://StepNeverStop.github.io/tags/overleaf/"/>
    
  </entry>
  
  <entry>
    <title>The Option-Critic Architecture</title>
    <link href="http://StepNeverStop.github.io/options-critic.html"/>
    <id>http://StepNeverStop.github.io/options-critic.html</id>
    <published>2020-04-14T10:57:10.000Z</published>
    <updated>2020-04-14T11:06:20.171Z</updated>
    
    <content type="html"><![CDATA[<p>这篇论文将Option-Critic这种端对端的分层强化学习算法推导出随机策略梯度的更新方式。</p><a id="more"></a><h1 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h1><p>论文地址：<a href="http://arxiv.org/abs/1609.05140" rel="external nofollow" target="_blank">http://arxiv.org/abs/1609.05140</a></p><h1 id="文中精要"><a href="#文中精要" class="headerlink" title="文中精要"></a>文中精要</h1><script type="math/tex; mode=display">Q_{\Omega}(s, \omega)=\sum_{a} \pi_{\omega, \theta}(a | s) Q_{U}(s, \omega, a)</script><p>$Q_{U}: \mathcal{S} \times \Omega \times \mathcal{A} \rightarrow \mathbb{R}$</p><script type="math/tex; mode=display">Q_{U}(s, \omega, a)=r(s, a)+\gamma \sum_{s^{\prime}} \mathrm{P}\left(s^{\prime} | s, a\right) U\left(\omega, s^{\prime}\right)</script><script type="math/tex; mode=display">U\left(\omega, s^{\prime}\right)=\left(1-\beta_{\omega, \vartheta}\left(s^{\prime}\right)\right) Q_{\Omega}\left(s^{\prime}, \omega\right)+\beta_{\omega, \vartheta}\left(s^{\prime}\right) V_{\Omega}\left(s^{\prime}\right)</script><script type="math/tex; mode=display">\mathrm{P}\left(s_{t+1}, \omega_{t+1} | s_{t}, \omega_{t}\right)=\sum_{a} \pi_{\omega_{t}, \theta}\left(a | s_{t}\right) \mathrm{P}\left(s_{t+1} | s_{t}, a\right) \left.\left(1-\beta_{\omega_{t}, \vartheta}\left(s_{t+1}\right)\right) \mathbf{1}_{\omega_{t}=\omega_{t+1}}+\beta_{\omega_{t}, \vartheta}\left(s_{t+1}\right) \pi_{\Omega}\left(\omega_{t+1} | s_{t+1}\right)\right)</script><script type="math/tex; mode=display">\frac{\partial Q_{\Omega}(s, \omega)}{\partial \theta} =\left(\sum_{a} \frac{\partial \pi_{\omega, \theta}(a | s)}{\partial \theta} Q_{U}(s, \omega, a)\right) +\sum_{a} \pi_{\omega, \theta}(a | s) \sum_{s^{\prime}} \gamma \mathrm{P}\left(s^{\prime} | s, a\right) \frac{\partial U\left(\omega, s^{\prime}\right)}{\partial \theta}</script><h1 id="伪代码"><a href="#伪代码" class="headerlink" title="伪代码"></a>伪代码</h1><p><img src="./options-critic/pseudo.png" alt=""></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;这篇论文将Option-Critic这种端对端的分层强化学习算法推导出随机策略梯度的更新方式。&lt;/p&gt;
    
    </summary>
    
      <category term="ReinforcementLearning" scheme="http://StepNeverStop.github.io/categories/ReinforcementLearning/"/>
    
    
      <category term="rl" scheme="http://StepNeverStop.github.io/tags/rl/"/>
    
  </entry>
  
  <entry>
    <title>蓝猫淘气三千问</title>
    <link href="http://StepNeverStop.github.io/rl3000questions.html"/>
    <id>http://StepNeverStop.github.io/rl3000questions.html</id>
    <published>2020-04-12T10:31:27.000Z</published>
    <updated>2020-04-27T13:29:04.620Z</updated>
    
    <content type="html"><![CDATA[<p>此篇博文用于记录学习RL或者实现RL方法过程中遇到疑难杂症及相应解决思路。</p><a id="more"></a><h1 id="待解决"><a href="#待解决" class="headerlink" title="待解决"></a>待解决</h1><ol><li>如果场景中存在多个图像输入，而且维度不一致，该如何处理呢？假设有汽车有前后两个摄像头，分辨率分别是$Vis_1=10\times10\times3$和$Vis_2=15\times5\times3$，那么该如何设计图像处理过程呢？是表示成$(CNN_1(Vis_1), CNN_2(Vis_2))$呢？还是表示成$CNN(Concat(Vis_1, Resize(Vis_2)))$呢？</li><li>如果智能体的动作既包括离散的也包括连续的，该如何处理？</li><li>在DQN系列中，经常提及过估计(over-estimation)问题，这个过估计究竟指的是什么过估计？怎样过估计？为何过估计？过估计有什么后果？</li></ol><h1 id="已解决"><a href="#已解决" class="headerlink" title="已解决"></a>已解决</h1><ol><li><p>什么是Semi-Markov Decision Process(SMDP)？</p><p><a href="https://zhuanlan.zhihu.com/p/47051292" rel="external nofollow" target="_blank">原链接</a>。分层强化学习里面，这里的上层策略其实是定义在一个SMDP上的，引用一下俞扬老师文章里面的一句话来解释一下。</p><blockquote><p>马尔可夫决策过程中，选择一个动作后，agent 会立刻根据状态转移方程$P$跳转到下一个状态，而在半马尔可夫决策过程(SMDP)中，当前状态到下一个状态的步数是个随机变量 $\tau$ ， 即在某个状态$s$下选择一个动作$a$后，经过$\tau$步才会以一个概率转移到下一个状态$s’$。 此时的状态转移概率是$s$和$\tau$的联合概率$P\left(s^{\prime}, \tau | s, a\right)$。</p></blockquote><p>文献：周文吉, and 俞扬. “分层强化学习综述.”<em>智能系统学报</em>12.5 (2017): 590-594.</p></li></ol>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;此篇博文用于记录学习RL或者实现RL方法过程中遇到疑难杂症及相应解决思路。&lt;/p&gt;
    
    </summary>
    
      <category term="ReinforcementLearning" scheme="http://StepNeverStop.github.io/categories/ReinforcementLearning/"/>
    
    
      <category term="rl" scheme="http://StepNeverStop.github.io/tags/rl/"/>
    
  </entry>
  
  <entry>
    <title>PaStaNet: Toward Human Activity Knowledge Engine</title>
    <link href="http://StepNeverStop.github.io/PaStaNet.html"/>
    <id>http://StepNeverStop.github.io/PaStaNet.html</id>
    <published>2020-04-11T17:23:03.000Z</published>
    <updated>2020-04-12T11:38:34.924Z</updated>
    
    <content type="html"><![CDATA[<p>这篇论文是上海交大<a href="http://mvig.sjtu.edu.cn/" rel="external nofollow" target="_blank">卢策吾</a>老师团队下<a href="https://dirtyharrylyl.github.io/" rel="external nofollow" target="_blank">李永露</a>博士在2020CVPR会议三连中中的其中一篇。方向为HOIs方向，即人物交互。</p><a id="more"></a><h1 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h1><p>论文地址：<a href="http://arxiv.org/abs/2004.00945" rel="external nofollow" target="_blank">http://arxiv.org/abs/2004.00945</a></p><p>作者开源的代码和数据集：<a href="http://hake-mvig.cn/" rel="external nofollow" target="_blank">http://hake-mvig.cn/</a></p><p>PaSta是Part State的缩写，它是<strong>细粒度动作语义标记</strong>(ﬁne-grained action semantic tokens)，是人类活动/行为的更精细的表达，比如一个人类的行为是开汽车，那么这个行为的part state就包括手握方向盘、脚踩油门等等，这种part state用三元组形式表示，比如：&#60;hand, hold, something&#62;。</p><p>这篇论文主要有两个比较大的贡献：</p><ol><li>建立了一个大型知识库PaStaNet（其实是HAKE数据集），目前标注了700w+局部状态。</li><li>设计了一个分层的动作识别模型（为什么分层呢？因为作者提到现有的基于图像的动作识别理解方法主要采取直接映射/端到端的方式，可能会遇到性能瓶颈。）<ol><li>第一层是Activity2Vec模型，用来从原始图片中提取PaSta特征，PaSta是组成多种人类行为的通用表示，比如一个PaSta是hold，那么开汽车时有hold方向盘，吃苹果时有hold苹果，两种不同的行为共享同样的PaSta；</li><li>第二层使用了PaSta-based Reasoning（PaSta-R，基于局部状态的推理）方法，用这种方法从第一层中识别的PaSta来推测图片中的人类行为活动。</li></ol></li></ol><p>下文中以下概念术语等同：</p><ul><li>PaStaNet——数据集</li><li>人类行为理解——动作识别</li><li>PaSta——局部状态</li></ul><h1 id="文中精要"><a href="#文中精要" class="headerlink" title="文中精要"></a>文中精要</h1><p>在大规模的基准中，基于<strong>实例层次的语义（instance-level semantics）</strong>使用one-stage从像素理解人类行为存在性能瓶颈，主要有以下几个原因：</p><ol><li><p>long-tail data distribution，长尾数据分布（少数类别有大部分数据，而多数类别只有小部分数据）</p><p><img src="./PaStaNet/Long_tailed_distribution.png" alt=""></p></li><li><p>complex visual patterns，复杂的视觉模式</p></li></ol><p>作者认为(argue that)在人类局部的语义层次上进行感知是一个非常有前景的方向，这种方式之前被忽略了。</p><p><strong>作者的核心思想是：人类动作由细粒度的原子主体部分状态（PaSta）组成。</strong></p><blockquote><p>Our core idea is that human instance actions are composed of ﬁne-grained atomic body part states.</p></blockquote><p><img src="./PaStaNet/fig1.png" alt=""></p><p>先识别PaSta再推理行为有什么好处呢？</p><ol><li><p>与简化理论(reductionism)有强烈的直接关系</p><blockquote><p>This lies in strong relationships with reductionism.</p></blockquote></li><li><p>可以帮助我们选择有区别的部分，忽略不相关的部分</p><blockquote><p>the part-level path can help us to pick up discriminative parts and disregard irrelevant ones.</p></blockquote></li><li><p>从人体局部编码知识是实现人类活动知识引擎的关键步骤</p><blockquote><p>encoding knowledge from human parts is a crucial step toward human activity knowledge engine.</p></blockquote></li><li><p>Reusability and Transferability——可重用性和可转移性，多个行为的局部状态存在共享，比如一个PaSta是hold，那么开汽车时有hold方向盘，吃苹果时有hold苹果，两种不同的行为共享同样的PaSta。因此，我们可以用更少的PaSta来描述和区分大量的行为。对于few-shot学习，可重用性可以极大地缓解其学习困难。</p><blockquote><p>PaSta are basic components of actions, their relationship can be in analogy with the amino acid and protein, letter and word, etc. Hence, PaSta are reusable, e.g., 〈 hand, hold, something 〉 is shared by various actions like “hold horse” and “eat apple”. Therefore, we get the capacity to describe and differentiate plenty of activities with a much smaller set of PaSta, i.e. one-time labeling and transferability. For fewshot learning, reusability can greatly alleviate its learning difﬁculty. Thus our approach shows signiﬁcant improvements, e.g. we boost 13.9 mAP on one-shot sets of HICO</p></blockquote></li><li><p>Interpretability——可解释性，当模型预测一个人在做什么时，我们很容易知道原因:它的身体的各个部分在做什么。</p><blockquote><p>we obtain not only more powerful activity representations, but also better interpretation. When the model predicts what a person is doing, we can easily know the reasons: what the body parts are doing.</p></blockquote></li></ol><h2 id="PaStaNet数据集"><a href="#PaStaNet数据集" class="headerlink" title="PaStaNet数据集"></a>PaStaNet数据集</h2><p>该数据集目前已经标注了11.8w张图片，包括28.5w个人物，25w个交互的实体对象（比如球之类的），72.4万个行为，以及700w个人类局部状态。</p><p>该数据集目前有156个行为分类，76个PaSta分类。</p><p>广泛的分析证明，一般来说，PaStaNet可以覆盖大部分的局部级知识，可以很好的概括大部分情况。</p><p><img src="./PaStaNet/fig6.png" alt=""></p><p>下图为数据集中的行为和交互物体类别。</p><p><img src="./PaStaNet/table6.png" alt=""></p><p>下图为数据集中的局部状态PaSta类别。</p><p><img src="./PaStaNet/table7.png" alt=""></p><h3 id="PaSta的定义"><a href="#PaSta的定义" class="headerlink" title="PaSta的定义"></a>PaSta的定义</h3><p>将人体解耦成十个部分：head, two upper arms, two hands, hip, two thighs, two feet，即</p><ol><li>头</li><li>左臂</li><li>右臂</li><li>左手</li><li>右手</li><li>臀部</li><li>左腿</li><li>右腿</li><li>左脚</li><li>右脚</li></ol><p>每一个PaSta表示目标局部部分的表示，比如hand可以是hold something, push something；head可以是watch something, eat something。注意，当一个人同时有多个行为动作，他的某个局部身体部位可以有多个PaSta。</p><h3 id="数据收集"><a href="#数据收集" class="headerlink" title="数据收集"></a>数据收集</h3><p>两种方式：</p><ol><li>通过众包收集以人为中心的行为图像(3万张图片，具有粗糙的活动标签)；</li><li>现有的设计良好的数据集(18.5万张)。</li></ol><p>其中的数据围绕丰富的语义本体论(semantic ontology)、多样性和行为的可变性构建。最终，收集了超过20万张的不同行为类别的图片。</p><h3 id="行为标签"><a href="#行为标签" class="headerlink" title="行为标签"></a>行为标签</h3><p>根据人类最常见的日常活动，与人和物的互动。从11.8万张图片中选择了156种行为，包括人与物体的互动和身体动作（包含bounding boxes）。</p><h3 id="身体局部的盒子"><a href="#身体局部的盒子" class="headerlink" title="身体局部的盒子"></a>身体局部的盒子</h3><blockquote><p>Estimation errors are addressed manually to ensure high-quality annotation. Each part box is centered with a joint, and the box size is pre-deﬁned by scaling the distance between the joints of the neck and pelvis. A joint with conﬁdence higher than 0.7 will be seen as visible. When not all joints can be detected, we use body knowledge-based rules. That is, if the neck or pelvis is invisible, we conﬁgure the part boxes according to other visible joint groups (head, main body, arms, legs), e.g., if only the upper body is visible, we set the size of the hand box to twice the pupil distance.</p></blockquote><h3 id="局部状态PaSta标注"><a href="#局部状态PaSta标注" class="headerlink" title="局部状态PaSta标注"></a>局部状态PaSta标注</h3><p>通过众包方式进行标注，共收到224159条标注上传。</p><p>过程如下：</p><ol><li>基于156种行为的动词，从WordNet选取200个PaSta动词。如果某个局部部位没有可以的状态，则描述为”no_action”；</li><li>为了找到最通用的PaSta（可以作为可转移的行为知识），邀请了来自不同背景的150名注释者来标注156个行为的1w张图片；</li><li>基于它们的注释，使用规范化的<strong>点对点互信息(Normalized Point-wise Mutual Information，NPMI，<em>Kenneth Ward Church and Patrick Hanks. Word association norms, mutual information, and lexicography. In Computational linguistics, 1990.</em>)</strong>来计算行为和PaSta之间的共生/共现关系，最后选择76个具有最高NPMI值的候选局部状态为PaSta集合；</li><li>以之前的1w张打了标签的图片为种子，自动生成其余图片的初始PaSta标签，然后另外210名注释者仅需要去检查这些标注即可；</li><li>考虑到一个人可能有多个动作，对于每个动作，分别标注其对应的10个PaSta。然后把所有动作的PaSta组合在一起；</li><li>为了确保质量，每幅图像都将被标注两次，并由自动程序和主管进行检查。</li></ol><p><img src="./PaStaNet/fig2.png" alt=""></p><p><strong>疑问：为什么ride bicycle与head look at的共现如此之低呢？</strong></p><h3 id="行为解析树"><a href="#行为解析树" class="headerlink" title="行为解析树"></a>行为解析树</h3><p>为了说明PaSta和行为之间的关系，作者使用它们的统计相关性来构建一个图：行为是根节点，PaSta是子节点，边是共现。</p><blockquote><p>To illustrate the relationships between PaSta and activities, we use their statistical correlations to construct a graph (Fig. 2): activities are root nodes, PaSta are son nodes and edges are co-occurrence.</p></blockquote><p>PaStaNet可以为实例级和局部级提供丰富的行为知识，并帮助构建大型行为解析树。</p><blockquote><p>PaStaNet can provide abundant activity knowledge for both instance and part levels and help construct a large-scale activity parsing tree</p></blockquote><p><img src="./PaStaNet/fig8.png" alt=""></p><p>作者将解析树表示为行为和PaSta的共现矩阵(看起来极其稀疏)。</p><p><img src="./PaStaNet/fig9.png" alt=""></p><h2 id="分层行为理解模型"><a href="#分层行为理解模型" class="headerlink" title="分层行为理解模型"></a>分层行为理解模型</h2><p>这一部分数学符号很多，而且似乎故意把符号设计的复杂，导致阅读理解起来有些不顺畅。</p><p>对于行为的识别，有两种模型：</p><ol><li><p>传统模式，采用直接映射。</p><script type="math/tex; mode=display">\mathcal{S}_{i n s t}=\mathcal{F}_{i n s t}\left(I, b_{h}, \mathcal{B}_{o}\right)</script><p>其中，$I$表示图像输入，$b_h$是人的box，$\mathcal{B}_{o}=\left\{b_{o}^{i}\right\}_{i=1}^{m}$是与人交互的物体的box，假设有$m$个物体。$\mathcal{S}_{i n s t}$代表实体级别的动作评分（评估结果）。</p></li><li><p>作者提出的PaStaNet模式，利用通用的局部知识，分成两步：</p><ol><li><p>PaSta局部状态识别和特征提取（其实是识别层之前的隐特征）</p><script type="math/tex; mode=display">f_{P a S t a}=\mathcal{R}_{A 2 V}\left(I, \mathcal{B}_{p}, b_{o}\right) = \left\{f_{P a S t a}^{(i)}\right\}_{i=1}^{10}</script><p>其中，$\mathcal{B}_{p}=\left\{b_{p}^{(i)}\right\}_{i}^{10}$是人的局部部位的box，使用<em>Pairwise body-part attention for recognizing human-object interactions. In ECCV, 2018</em>自动生成。$\mathcal{R}_{A 2 V}(\cdot)$表示Activity2Vec模型，用于提取PaSta的特征表示，</p></li><li><p>PaSta-Based推理（PaSta-R），从局部状态推理行为语义</p><script type="math/tex; mode=display">\mathcal{S}_{p a r t}=\mathcal{F}_{P a S t a-R}\left(f_{P a S t a}, f_{o}\right)</script><p>其中，$\mathcal{F}_{P a S t a-R}(\cdot)$代表PaSta-R方法，$f_{o}$是物体的特征表示，$\mathcal{S}_{p a r t}$是局部状态层面的动作评分。<em>注意，如果场景中人没有与物进行交互，比如”跳舞“这个动作，那么使用图像的ROI池化特征来表示$f_o$。如果场景中存在多个交互物体，则依次处理human-object pair$\left(f_{P a S t a}, f_{o}^{(i)}\right)$，并且声称各自独立的Activity2Vec Embedding</em>。</p></li></ol></li></ol><p><img src="./PaStaNet/fig3.png" alt=""></p><p>上图为PaSta的识别与特征表示部分的框架图。</p><p>识别部分主要为红色线条部分，特征表示部分主要为蓝色线条部分。</p><h3 id="局部状态PaSta识别"><a href="#局部状态PaSta识别" class="headerlink" title="局部状态PaSta识别"></a>局部状态PaSta识别</h3><p>这部分的输入为$I, \mathcal{B}_{p}, b_{o}$，输出为局部状态的视觉特征$f_{PaSta}^{V}$和识别结果$P_{PaSta}$。</p><p>对于输入，$\mathcal{B}_{p}, b_{o}$都是使用在COCO数据集上预训练的Faster R-CNN做特征提取：</p><ul><li>对于物体$b_o$，$b_o\rightarrow Faster R-CNN \rightarrow f_o$，如果图片内不存在与物体进行交互，则使用图像的特征，即$I\rightarrow Faster R-CNN \rightarrow f_c \rightarrow f_o$</li><li>对于人的身体的每一个部位（共10个）$b_{p}^{(i)}$，$b_{p}^{(i)} \rightarrow Faster R-CNN \rightarrow f_{p}^{(i)}$</li></ul><p>得到特征表示后，首先输入到一个被称为Part Relevance Predictor的结构中，去计算每一个部位的attention，这个PRP结构由全连接组成，最后激活为softmax函数，给每一个局部部位特征输出一个注意力权重：</p><script type="math/tex; mode=display">a_{i}=\mathcal{P}_{p a}\left(f_{p}^{(i)}, f_{o}\right)</script><p>其中$\mathcal{P}_{p a}(\cdot)$即是局部注意力预测器。<strong>在这里，我感觉这个注意力权重应该指的是某个身体部位与物体的相关性，比如，手跟茶杯很相关，而脚和苹果则不太相关。</strong>然后，将注意力权重与原始局部特征表示进行加权：</p><script type="math/tex; mode=display">f_{p}^{(i) \star}=f_{p}^{(i)} \times a_{i}</script><p>接下来进行局部状态PaSta的分类/识别，此时将$f_{p}^{(i) \star}$与$f_o$进行concat操作之后，传入max池化层，以及两层512的全连接，最终获得PaSta的分类结果$\mathcal{S}_{P a S t a}^{(i)}$。<strong>这里的$\mathcal{S}$应该是logits，而$P_{PaSta}$表示概率。</strong></p><p><em>注意，这里存在一个身体部位有多种状态的可能，比如头部可以同时进行”吃”和”看”的动作，因此是一个多标签分类任务。</em></p><p>识别部分的交叉熵损失函数如下：</p><script type="math/tex; mode=display">\mathcal{L}_{P a S t a}=\sum_{i}^{10}\left(\mathcal{L}_{P a S t a}^{(i)}+\mathcal{L}_{a t t}^{(i)}\right)</script><h3 id="Activity2Vec"><a href="#Activity2Vec" class="headerlink" title="Activity2Vec"></a>Activity2Vec</h3><p>这一部分的输入为局部状态的视觉特征$f_{PaSta}^{V}$、识别结果$P_{PaSta}$和PaSta的语言特征$f_{B e r t}^{(i, k)}$，输出为PaSta的最终特征表示$f_{PaSta}$。</p><blockquote><p>With PaStaNet, we convert a human instance into a vector consisting of PaSta representations. Activity2Vec extracts part-level semantic representation via PaSta recognition and combines its language representation. Since PaSta encodes common knowledge of activities, Activity2Vec works as a general feature extractor for both seen and unseen activities.</p></blockquote><p>Activity2Vec将一个人类实例转换为一个由PaSta表示组成的向量。通过局部状态识别提取局部层次的语义表示，并且与该局部状态的语言表示相结合。</p><p>在这一环节的主要任务是将局部状态PaSta的语义知识嵌入到它的特征向量表示中去，那么，如何结合呢？</p><p>对于图像特征，在上一部分已经获得，提取PaSta的分类结果前一层的隐状态即可，$\color{red}{f_{\text {PaSta}}^{V(i)} \in \mathbb{R}^{512}}$。</p><p>对于语言特征，作者使用<strong>BERT-Base预训练模型</strong>先将数据集中的token预转换为$\color{red}{f_{B e r t}^{(i, k)} \in \mathbb{R}^{2304}}$，并且在整个过程中保持不变。token指的是三元组&#60;part, verb, object&#62;，object来自目标检测。所有的token即$\left\{t_{p}^{(i, k)}, t_{v}^{(i, k)}, t_{o}^{(i, k)}\right\}_{k=1}^{n}$，$i$代表身体部位的数量，这里为10，$n$代表每一个部位具有的PaSta数量，其中的每个$t$都是768的向量长度。</p><script type="math/tex; mode=display">f_{B e r t}^{(i, k)}=\mathcal{R}_{B e r t}\left(t_{p}^{(i, k)}, t_{v}^{(i, k)}, t_{o}^{(i, k)}\right)</script><script type="math/tex; mode=display">f_{B e r t}^{(i)} \in \mathbb{R}^{2304 * n}</script><p>将部分的BERT表示与该部分的分类结果相乘，即PaSta的语言特征表示：</p><script type="math/tex; mode=display">f_{P a S t a}^{L(i)}=f_{B e r t}^{(i)} \times P_{P a S t a}^{(i)}, \text { where } P_{P a S t a}^{(i)}=\operatorname{Sigmoid}\left(\mathcal{S}_{P a S t a}^{(i)}\right) \in \mathbb{R}^{n}</script><script type="math/tex; mode=display">P_{P a S t a}=\left\{P_{P a S t a}^{(i)}\right\}_{i=1}^{10}</script><script type="math/tex; mode=display">f_{P a S t a}^{L(i)} \in \mathbb{R}^{2304 * n}</script><p><strong>最后</strong>，池化、resize语言特征$f_{P a S t a}^{L(i)}$后再与图像特征$f_{PaSta}^{V}$concat即获得最终的PaSta特征表示$f_{P a S t a}^{(i)} \in \mathbb{R}^{m}$。输出的$f_{\text {PaSta}}=\left\{f_{\text {PaSta}}^{(i)}\right\}_{i=1}^{10}$是局部级别的行为特征表示，可用于下游任务，像行为检测，标题生成等等。</p><h3 id="PaSta-R"><a href="#PaSta-R" class="headerlink" title="PaSta-R"></a>PaSta-R</h3><p>这一部分主要是从局部状态的特征表示推断出图片中人的行为。其输入为特征表示$f_{PaSta}$，输出为动作评分$\mathcal{S}$。</p><blockquote><p>A Part State Based Reasoning method (PaSta-R) is further presented. We construct a Hierarchical Activity Graph consisting of human instance and part semantic representations, and infer the activities by combining both instance and part level sub-graph states.</p></blockquote><p>作者构造了一个由人类实例和局部语义表示组成的层次行为图（Hierarchical Activity Graph），并结合实例和局部层次子图状态来推断行为。</p><p><img src="./PaStaNet/fig4.png" alt=""></p><p>HAG如上图中间所示，节点为局部的状态特征或者物体的特征，边分两种，第一种是body part与object的边，表示为$e_{p o}=\left(v_{p}^{i}, v_{o}\right) \in \mathcal{V}_{p} \times \mathcal{V}_{o}$，第二种是body part与body part的边，表示为$e_{p p}^{i j}=\left(v_{p}^{i}, v_{p}^{j}\right) \in \mathcal{V}_{p} \times \mathcal{V}_{p}$。<strong><em>说实话，边究竟是如何表示的，完全没有看懂-.-</em></strong>。</p><p>作者的目标是解析HAG，然后推理出图像中的行为。即</p><script type="math/tex; mode=display">\mathcal{S}_{p a r t}=\mathcal{F}_{P a S t a-R}\left(f_{P a S t a}, f_{o}\right)</script><p>作者提出了三种结构和三种方式，如下图所示：</p><p><img src="./PaStaNet/fig10.png" alt=""></p><p>三种结构：</p><ol><li>Linear Combination，说白了就是一层全连接，激活为softmax；</li><li>MLP，说白了就是两层1024单元全连接（使用非线性激活函数），最后一层激活为softmax；</li><li>Graph Convolution Network，GCN提取全局图特征，然后接MLP输出分类结果。</li></ol><p>三种方式：</p><ol><li>上图(a)所示，将$f_{P a S t a}, f_{o}$直接concat然后输入后续网络；</li><li>上图(b)所示，按身体部位逐步输入到LSTM网络，改造成序列模型。有两种输入方式，1乱序，2固定顺序（比如从头到脚），作者说固定顺序更好；</li><li>上图(c)所示，将部位特征分层组合，例如：<ol><li>在第一层将左手左上臂特征合并为左臂，左脚左大腿特征合并为左腿，……，然后传入全连接进一步提取特征；</li><li>将头、胳膊等合并为上肢，臀、腿等合并为下肢，……，然后传入全连接进行进一步特征提取；</li><li>上下肢合并为整体，然后传入全连接，再接后续网络。</li></ol></li></ol><p>如何得到最后的分类结果呢？作者提出两种方式：</p><ol><li>early fusion——前融合，将实例层次的语义特征表示$f_{inst}$与PaSta特征表示、物体特征表示结合后再做PaSta-R推理；</li><li>late fusion——后融合，融合实例层次的分类结果和局部层次的分类结果，即$\mathcal{S}=\mathcal{S}_{i n s t}+\mathcal{S}_{p a r t}$。作者说，实验下来，这种方式效果更好。</li></ol><p>最终，整个框架的交叉熵损失函数为：</p><script type="math/tex; mode=display">\mathcal{L}_{\text {total}}=\mathcal{L}_{\text {PaSta}}+\mathcal{L}_{\text {cls}}^{\text {PaSta}}+\mathcal{L}_{\text {cls}}^{\text {inst}}</script><h1 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h1><p>监督学习中，PaStaNet在HICO完整数据集上提升了6.4(16%)mAP，在HICO one-shot数据集上提升了13.9mAP。</p><p>迁移学习中，PaStanet在V-COCO数据集上提升了3.2mAP，在基于图像的AVA数据集上提升了4.2mAP，在HICO-DET数据集上提升了3.2mAP。</p><h2 id="类比实验"><a href="#类比实验" class="headerlink" title="类比实验"></a>类比实验</h2><p>从MNIST数据集中采集0-9数字（28X28X1），组成（128X128X1）的图片，每个图片包含3-5个数字。将数字类比为身体的局部部位，将行为设置为图片中最大的两个数字之和。图片中所有数字的union box代表一个人。为了模仿任务的移动特征，数字随机分布在图像中，而且给图像加入了高斯噪声。</p><p>最终实验结果（准确率）：</p><ul><li>端到端，10.0%</li><li>前融合：43.7%</li><li>后融合：<strong>44.2%</strong></li><li>不融合：41.4%</li></ul><p><img src="./PaStaNet/fig5.png" alt=""></p><h2 id="Image-based-Activity-Recognition"><a href="#Image-based-Activity-Recognition" class="headerlink" title="Image-based Activity Recognition"></a>Image-based Activity Recognition</h2><p><img src="./PaStaNet/table1.png" alt=""></p><h2 id="Instance-based-Activity-Detection"><a href="#Instance-based-Activity-Detection" class="headerlink" title="Instance-based Activity Detection"></a>Instance-based Activity Detection</h2><p><img src="./PaStaNet/table2.png" alt=""></p><h2 id="Transfer-Learning-with-Activity2Vec"><a href="#Transfer-Learning-with-Activity2Vec" class="headerlink" title="Transfer Learning with Activity2Vec"></a>Transfer Learning with Activity2Vec</h2><p><img src="./PaStaNet/table3.png" alt=""></p><p><img src="./PaStaNet/table4.png" alt=""></p><h2 id="可视化结果"><a href="#可视化结果" class="headerlink" title="可视化结果"></a>可视化结果</h2><p><img src="./PaStaNet/fig14.png" alt=""></p><p>图中蓝、绿、红分别指示身体部位、局部行为、交互物体。作者发现他们的模型能够检测各种行为，包括与各种对象的交互。</p><h1 id="mAP"><a href="#mAP" class="headerlink" title="mAP"></a>mAP</h1><p>mAP：mean Average Precision，平均的平均精度（两个平均）。先是类内求平均精确度，再是对所有类别再求平均精确度。</p><p>目标检测任务中将目标的分类结果分成四类（正即是真，负即是假）：</p><ol><li>TP——True Positive，正识别为正；</li><li>FP——False Positive，负识别为正；</li><li>TN——True Negative，负识别为负；</li><li>FN——False Negative，负识别为正</li></ol><p>准确率Precision——<strong>识别为正</strong>的数据中，真实为正的：</p><script type="math/tex; mode=display">P=\frac{T P}{T P+F P}=\frac{T P}{N_{\text {detection}}}</script><p>召回率Recall——<strong>原始为正</strong>的数据中，识别为正的：</p><script type="math/tex; mode=display">R=\frac{T P}{T P+F N}=\frac{T P}{N_{g t}}</script><p>平均精度AP即是在一组召回率阈值[0, 1]中，根据召回率计算相应准确率，然后准确率取平均。比如设置11个等间隔召回率阈值[0, 0.2, …, 1]，那么AP的计算公式如下：</p><script type="math/tex; mode=display">\begin{aligned}A P=& \frac{1}{11} \sum_{r \in\{0,0.1, \ldots, 1.0\}} \rho_{\text {interp}}(r) \\& \rho_{\text {interp}}(r)=\max _{\hat{r}: \hat{r} \geqslant r}(\hat{r})\end{aligned}</script><p>实际上就是对于每个Recall值下的Precision，取所有比当前值大的Recall对应的Precision的最大值作为当前Recall值下的Precision。</p><p>mAP是多个AP值的均值，AP衡量的是学出来的模型在每个类别上的好坏，mAP衡量的是学出的模型在所有类别上的好坏。</p><h1 id="结论"><a href="#结论" class="headerlink" title="结论"></a>结论</h1><blockquote><p>In this paper, to make a step toward human activity knowledge engine, we construct PaStaNet to provide novel body part-level activity representation (PaSta). Meanwhile, a knowledge transformer Activity2Vec and a part-based reasoning method PaSta-R are proposed. PaStaNet brings in interpretability and new possibility for activity understanding. It can effectively bridge the semantic gap between pixels and activities. With PaStaNet, we signiﬁcantly boost the performance in supervised and transfer learning tasks, especially under few-shot circumstances. In the future, we plan to enrich our PaStaNet with spatio-temporal PaSta.</p></blockquote><h1 id="引用"><a href="#引用" class="headerlink" title="引用"></a>引用</h1><ol><li><p><a href="https://blog.csdn.net/xiezongsheng1990/article/details/89608923?depth_1-utm_source=distribute.pc_relevant.none-task-blog-BlogCommendFromBaidu-1&amp;utm_source=distribute.pc_relevant.none-task-blog-BlogCommendFromBaidu-1" rel="external nofollow" target="_blank">目标检测测评指标——mAP</a></p></li><li><p><a href="http://blog.sina.com.cn/s/blog_9db078090102whzw.html" rel="external nofollow" target="_blank">多标签图像分类任务的评价方法-mAP</a></p></li></ol>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;这篇论文是上海交大&lt;a href=&quot;http://mvig.sjtu.edu.cn/&quot; rel=&quot;external nofollow&quot; target=&quot;_blank&quot;&gt;卢策吾&lt;/a&gt;老师团队下&lt;a href=&quot;https://dirtyharrylyl.github.io/&quot; rel=&quot;external nofollow&quot; target=&quot;_blank&quot;&gt;李永露&lt;/a&gt;博士在2020CVPR会议三连中中的其中一篇。方向为HOIs方向，即人物交互。&lt;/p&gt;
    
    </summary>
    
      <category term="DeepLearning" scheme="http://StepNeverStop.github.io/categories/DeepLearning/"/>
    
    
      <category term="dl" scheme="http://StepNeverStop.github.io/tags/dl/"/>
    
  </entry>
  
  <entry>
    <title>Hierarchical Deep Reinforcement Learning: Integrating Temporal Abstraction and Intrinsic Motivation</title>
    <link href="http://StepNeverStop.github.io/h-dqn.html"/>
    <id>http://StepNeverStop.github.io/h-dqn.html</id>
    <published>2020-04-11T04:35:31.000Z</published>
    <updated>2020-04-11T04:40:15.274Z</updated>
    
    <content type="html"><![CDATA[<a id="more"></a><h1 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h1><p>论文地址：<a href="http://arxiv.org/abs/1604.06057" rel="external nofollow" target="_blank">http://arxiv.org/abs/1604.06057</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;a id=&quot;more&quot;&gt;&lt;/a&gt;
&lt;h1 id=&quot;简介&quot;&gt;&lt;a href=&quot;#简介&quot; class=&quot;headerlink&quot; title=&quot;简介&quot;&gt;&lt;/a&gt;简介&lt;/h1&gt;&lt;p&gt;论文地址：&lt;a href=&quot;http://arxiv.org/abs/1604.06057&quot; rel=&quot;
      
    
    </summary>
    
      <category term="ReinforcementLearning" scheme="http://StepNeverStop.github.io/categories/ReinforcementLearning/"/>
    
    
      <category term="rl" scheme="http://StepNeverStop.github.io/tags/rl/"/>
    
  </entry>
  
  <entry>
    <title>Deep Exploration via Bootstrapped DQN</title>
    <link href="http://StepNeverStop.github.io/bootstrapped-dqn.html"/>
    <id>http://StepNeverStop.github.io/bootstrapped-dqn.html</id>
    <published>2020-04-11T04:35:24.000Z</published>
    <updated>2020-04-11T04:39:56.646Z</updated>
    
    <content type="html"><![CDATA[<a id="more"></a><h1 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h1><p>论文地址：<a href="http://arxiv.org/abs/1602.04621" rel="external nofollow" target="_blank">http://arxiv.org/abs/1602.04621</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;a id=&quot;more&quot;&gt;&lt;/a&gt;
&lt;h1 id=&quot;简介&quot;&gt;&lt;a href=&quot;#简介&quot; class=&quot;headerlink&quot; title=&quot;简介&quot;&gt;&lt;/a&gt;简介&lt;/h1&gt;&lt;p&gt;论文地址：&lt;a href=&quot;http://arxiv.org/abs/1602.04621&quot; rel=&quot;
      
    
    </summary>
    
      <category term="ReinforcementLearning" scheme="http://StepNeverStop.github.io/categories/ReinforcementLearning/"/>
    
    
      <category term="rl" scheme="http://StepNeverStop.github.io/tags/rl/"/>
    
  </entry>
  
  <entry>
    <title>Emergence of Locomotion Behaviours in Rich Environments</title>
    <link href="http://StepNeverStop.github.io/dppo.html"/>
    <id>http://StepNeverStop.github.io/dppo.html</id>
    <published>2020-04-11T04:35:16.000Z</published>
    <updated>2020-04-11T06:15:45.331Z</updated>
    
    <content type="html"><![CDATA[<p>这篇论文主要提出了DPPO——Distributed PPO。</p><a id="more"></a><h1 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h1><p>论文地址：<a href="http://arxiv.org/abs/1707.02286" rel="external nofollow" target="_blank">http://arxiv.org/abs/1707.02286</a></p><h1 id="文中精要"><a href="#文中精要" class="headerlink" title="文中精要"></a>文中精要</h1><p>作者提到PG算法通常具有高方差，而且策略对于超参数的选择十分敏感。很多种方法</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;这篇论文主要提出了DPPO——Distributed PPO。&lt;/p&gt;
    
    </summary>
    
      <category term="ReinforcementLearning" scheme="http://StepNeverStop.github.io/categories/ReinforcementLearning/"/>
    
    
      <category term="rl" scheme="http://StepNeverStop.github.io/tags/rl/"/>
    
  </entry>
  
  <entry>
    <title>Tensorflow2.0中的高斯分布及其概率</title>
    <link href="http://StepNeverStop.github.io/tf2-gaussian-distribution.html"/>
    <id>http://StepNeverStop.github.io/tf2-gaussian-distribution.html</id>
    <published>2020-04-08T02:50:12.000Z</published>
    <updated>2020-04-08T12:03:09.830Z</updated>
    
    <content type="html"><![CDATA[<p>此篇博文用于记录和描述一些高斯分布的基本特性以及在tensorflow2.0中的不同之处。</p><a id="more"></a><h1 id="对角协方差高斯分布"><a href="#对角协方差高斯分布" class="headerlink" title="对角协方差高斯分布"></a>对角协方差高斯分布</h1><p>对角协方差矩阵： Diagonal Covariance Matrix</p><p>多元高斯分布：multivariate Gaussian distribution</p><p>拥有对角协方差的多元高斯分布，其变量的概率密度等于各个变量的一元高斯概率密度之积。</p><p>假设对角协方差矩阵是如下形式：</p><script type="math/tex; mode=display">\Sigma =\left(  \begin{array}{cccc}    \sigma_{1} & 0 & \cdots & 0 \\    0 & \sigma_{2} & \cdots & 0 \\    \vdots & \vdots & \ddots & \vdots \\    0 & 0 & \cdots & \sigma_{k}  \end{array}\right)</script><p>那么，多元高斯函数的概率密度函数被定义为：</p><script type="math/tex; mode=display">f_{x}\left(x_{1}, \ldots, x_{k}\right)=\frac{\exp \left(-\frac{1}{2}(\vec{x}-\vec{\mu})^{T} \Sigma^{-1}(\vec{x}-\vec{\mu})\right)}{\sqrt{|2 \pi \Sigma|}}</script><p>变量替换：</p><script type="math/tex; mode=display">\vec{y}=\vec{x}-\vec{\mu}</script><p>对角协方差的逆可以表示为：</p><script type="math/tex; mode=display">\begin{aligned}\Sigma^{-1} &=\left(\begin{array}{cccc}\sigma_{1}^{2} & 0 & \cdots & 0 \\0 & \sigma_{2}^{2} & \cdots & 0 \\\vdots & \vdots & \ddots & \vdots \\0 & 0 & \cdots & \sigma_{k}^{2}\end{array}\right)^{-1} \\&=\left(\begin{array}{cccc}\frac{1}{\sigma_{1}^{2}} & 0 & \cdots & 0 \\0 & \frac{1}{\sigma_{2}^{2}} & \cdots & 0 \\\vdots & \vdots & \ddots & \vdots \\0 & 0 & \cdots & \frac{1}{\sigma_{k}^{2}}\end{array}\right)\end{aligned}</script><p>代入上面式子，可以得到：</p><script type="math/tex; mode=display">\begin{align}    f_x(x_1, \ldots, x_k) = \frac{\exp \Big(-\frac{1}{2} ( \frac{y_1^2}{\sigma_1^2} + \frac{y_2^2}{\sigma_2^2} + \ldots + \frac{y_k^2}{\sigma_k^2} ) \Big)}{\sqrt{(2\pi)^{k}|\Sigma|}}\end{align}</script><p>其中，分母的变化是由于矩阵秩的性质。对角协方差矩阵的秩可以展开写成：</p><script type="math/tex; mode=display">\begin{align}    |\Sigma| &=        \begin{vmatrix}            \sigma_1^2 & 0 & \cdots & 0 \\            0 & \sigma_2^2 & \cdots & 0 \\            \vdots & \vdots & \ddots & \vdots \\            0 & 0 & \cdots & \sigma_k^2 \\        \end{vmatrix} \\        &= \sigma_1^2 \cdot \sigma_2^2 \cdots \sigma_k^2        \end{align} %]]></script><p>最后，代入原概率密度方程并化简：</p><script type="math/tex; mode=display">\begin{align}    f_x(x_1, \ldots, x_k) &= \frac{\exp \Big(-\frac{1}{2} ( \frac{y_1^2}{\sigma_1^2} + \frac{y_2^2}{\sigma_2^2} + \ldots + \frac{y_k^2}{\sigma_k^2} ) \Big)}{\sqrt{(2\pi)^{k}\sigma_1^2 \cdot \sigma_2^2 \cdots \sigma_k^2}} \\    &= \frac{\exp \Big(-\frac{1}{2} ( \frac{y_1^2}{\sigma_1^2} + \frac{y_2^2}{\sigma_2^2} + \ldots + \frac{y_k^2}{\sigma_k^2} ) \Big)}{\sqrt{2\pi\sigma_1^2 \cdot 2\pi\sigma_2^2 \cdots 2\pi\sigma_k^2}} \\    &= \frac{\exp\Big(-\frac{y_1^2}{2\sigma_1^2} \Big)}{\sqrt{2\pi\sigma_1^2}}  \cdot \frac{\exp\Big(-\frac{y_2^2}{2\sigma_2^2} \Big)}{\sqrt{2\pi\sigma_2^2}}  \cdots \frac{\exp\Big(-\frac{y_k^2}{2\sigma_k^2} \Big)}{\sqrt{2\pi\sigma_k^2}} \\    &= \frac{\exp\Big(-\frac{(x_1-\mu_1)^2}{2\sigma_1^2} \Big)}{\sqrt{2\pi\sigma_1^2}}  \cdot \frac{\exp\Big(-\frac{(x_2-\mu_2)^2}{2\sigma_2^2} \Big)}{\sqrt{2\pi\sigma_2^2}}  \cdots \frac{\exp\Big(-\frac{(x_k-\mu_k)^2}{2\sigma_k^2} \Big)}{\sqrt{2\pi\sigma_k^2}} \\    &= f_1(x_1) \cdot f_2(x_2) \cdots f_k(x_k)\end{align} %]]></script><p>这样就得到了，对角协方差矩阵的多元高斯分布，其变量的联合概率密度等于各个变量独立高斯分布概率密度的连积。</p><h1 id="TensorFlow-2-0-Gaussian-Functions"><a href="#TensorFlow-2-0-Gaussian-Functions" class="headerlink" title="TensorFlow 2.0 Gaussian Functions"></a>TensorFlow 2.0 Gaussian Functions</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">import</span> tensorflow_probability <span class="keyword">as</span> tfp</span><br><span class="line"></span><br><span class="line">dt = tf.float32</span><br><span class="line"></span><br><span class="line">mu = tf.constant([<span class="number">1.</span>, <span class="number">2.</span>], dtype=dt)<span class="comment"># 均值</span></span><br><span class="line">sigma = tf.constant([<span class="number">1.</span>, <span class="number">2.</span>], dtype=dt)<span class="comment"># 标准差</span></span><br><span class="line">covariance = tf.constant([<span class="comment"># 协方差矩阵</span></span><br><span class="line">  [<span class="number">1.</span>, <span class="number">0.</span>],</span><br><span class="line">  [<span class="number">0.</span>, <span class="number">4.</span>]</span><br><span class="line">])</span><br><span class="line">x = tf.constant([<span class="number">0.92</span>, <span class="number">2.03</span>], dtype=dt)<span class="comment"># 模拟一个样本</span></span><br></pre></td></tr></table></figure><p>测试一下TF中几个不同高斯分布的1. 采样形式；2. 样本概率的表示。定义一个函数：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">test_gaussian</span><span class="params">(dist, y)</span>:</span></span><br><span class="line">  <span class="string">'''</span></span><br><span class="line"><span class="string">  param dist: 传入的高斯分布</span></span><br><span class="line"><span class="string">  param y: 传入的样本</span></span><br><span class="line"><span class="string">  '''</span></span><br><span class="line">  x = dist.sample()</span><br><span class="line">  print(x)<span class="comment"># 输出分布的采样</span></span><br><span class="line">  p = dist.prob(x)</span><br><span class="line">  print(p)<span class="comment"># 输出采样样本的概率</span></span><br><span class="line">  p_y = dist.prob(y)</span><br><span class="line">  print(<span class="string">'prob: '</span>, p_y)<span class="comment"># 输出样本的概率</span></span><br><span class="line">  print(<span class="string">'sum_prob: '</span>, tf.reduce_sum(p_y))<span class="comment"># 输出样本的概率之和</span></span><br><span class="line">  print(<span class="string">'prod_prob: '</span>, tf.reduce_prod(p_y))<span class="comment"># 输出样本的概率之积</span></span><br><span class="line">  log_p_y = dist.log_prob(y)</span><br><span class="line">  print(<span class="string">'log_prob: '</span>, log_p_y)<span class="comment"># 输出样本的概率对数</span></span><br><span class="line">  print(<span class="string">'sum_log_prob: '</span>, tf.reduce_sum(log_p_y))<span class="comment"># 输出样本的概率对数之和</span></span><br><span class="line">  print(<span class="string">'prod_log_prob: '</span>, tf.reduce_prod(log_p_y))<span class="comment"># 输出样本的概率对数之积</span></span><br></pre></td></tr></table></figure><h2 id="Normal"><a href="#Normal" class="headerlink" title="Normal"></a>Normal</h2><p><code>test_gaussian(tfp.distributions.Normal(loc=mu, scale=sigma), x)</code>，其参数中的scale为标准差。</p><p>输出为：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">tf.Tensor([-0.24812889  2.8150756 ], shape=(2,), dtype=float32)</span><br><span class="line">tf.Tensor([0.18307646 0.1835755 ], shape=(2,), dtype=float32)</span><br><span class="line">prob:  tf.Tensor([0.3976677  0.19944869], shape=(2,), dtype=float32)</span><br><span class="line">sum_prob:  tf.Tensor(0.5971164, shape=(), dtype=float32)</span><br><span class="line">prod_prob:  tf.Tensor(0.07931431, shape=(), dtype=float32)</span><br><span class="line">log_prob:  tf.Tensor([-0.9221385 -1.6121982], shape=(2,), dtype=float32)</span><br><span class="line">sum_log_prob:  tf.Tensor(-2.5343368, shape=(), dtype=float32)</span><br><span class="line">prod_log_prob:  tf.Tensor(1.4866701, shape=(), dtype=float32)</span><br></pre></td></tr></table></figure><p>这个分布为各个变量的独立高斯分布，有$n$个变量，则采样出$n$个值，其概率为每个变量各自的概率，如$n=2$，上面结果给出概率为：prob:  tf.Tensor([0.3976677  0.19944869], shape=(2,), dtype=float32)。</p><h2 id="MultivariateNormalDiag"><a href="#MultivariateNormalDiag" class="headerlink" title="MultivariateNormalDiag"></a>MultivariateNormalDiag</h2><p><code>test_gaussian(tfp.distributions.MultivariateNormalDiag(loc=mu, scale_diag=sigma), x)</code>，其参数中的scale_diag为协方差矩阵对角线的平方根，也就是每个变量对应的标准差。</p><p>输出为：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">tf.Tensor([2.550062  1.5906484], shape=(2,), dtype=float32)</span><br><span class="line">tf.Tensor(0.02343988, shape=(), dtype=float32)</span><br><span class="line">prob:  tf.Tensor(0.07931432, shape=(), dtype=float32)</span><br><span class="line">sum_prob:  tf.Tensor(0.07931432, shape=(), dtype=float32)</span><br><span class="line">prod_prob:  tf.Tensor(0.07931432, shape=(), dtype=float32)</span><br><span class="line">log_prob:  tf.Tensor(-2.5343366, shape=(), dtype=float32)</span><br><span class="line">sum_log_prob:  tf.Tensor(-2.5343366, shape=(), dtype=float32)</span><br><span class="line">prod_log_prob:  tf.Tensor(-2.5343366, shape=(), dtype=float32)</span><br></pre></td></tr></table></figure><p>这个分布于各变量独立的高斯分布相同，其协方差矩阵为对角矩阵，与Normal不同的是，其概率为每个变量各自概率之积，即联合概率。其概率对数为每个变量各自概率对数之和。</p><p>prob = Normal: prod_prob</p><p>log_prob = Normal: sum_log_prob</p><h2 id="MultivariateNormalFullCovariance"><a href="#MultivariateNormalFullCovariance" class="headerlink" title="MultivariateNormalFullCovariance"></a>MultivariateNormalFullCovariance</h2><p><code>test_gaussian(tfp.distributions.MultivariateNormalFullCovariance(loc=mu, covariance_matrix=covariance), x)</code>，其参数中的covariance_matrix为协方差矩阵，因此如果使用<code>MultivariateNormalDiag</code>并指定对角线参数为<code>scale_diag=[1,2]</code>，那么实现相同的分布使用<code>MultivariateNormalFullCovariance</code>应指定协方差矩阵参数为<code>covariance_matrix=[[1,0],[0,2]]</code>。</p><p>输出为：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">tf.Tensor([3.1393874 1.2191284], shape=(2,), dtype=float32)</span><br><span class="line">tf.Tensor(0.007478423, shape=(), dtype=float32)</span><br><span class="line">prob:  tf.Tensor(0.07931432, shape=(), dtype=float32)</span><br><span class="line">sum_prob:  tf.Tensor(0.07931432, shape=(), dtype=float32)</span><br><span class="line">prod_prob:  tf.Tensor(0.07931432, shape=(), dtype=float32)</span><br><span class="line">log_prob:  tf.Tensor(-2.5343366, shape=(), dtype=float32)</span><br><span class="line">sum_log_prob:  tf.Tensor(-2.5343366, shape=(), dtype=float32)</span><br><span class="line">prod_log_prob:  tf.Tensor(-2.5343366, shape=(), dtype=float32)</span><br></pre></td></tr></table></figure><p>这个分布为多个变量的联合高斯分布，如果设置协方差矩阵为对角矩阵，且为<code>MultivariateNormalDiag</code>对角参数的平方，则两个分布一致。</p><h2 id="TruncatedNormal"><a href="#TruncatedNormal" class="headerlink" title="TruncatedNormal"></a>TruncatedNormal</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">tfp.distributions.TruncatedNormal(</span><br><span class="line">    loc, scale, low, high, validate_args=<span class="keyword">False</span>, allow_nan_stats=<span class="keyword">True</span>,</span><br><span class="line">    name=<span class="string">'TruncatedNormal'</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure><p>这个分布相比<code>Normal</code>多了两个参数：low和high，如果样本＞high或者＜low，则其概率为0，对数概率为-inf。使用这个分布采样的样本也在low和high之间。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">dist = tfp.distributions.TruncatedNormal(loc=[<span class="number">0.</span>, <span class="number">1.</span>], scale=<span class="number">1.0</span>, low=[<span class="number">-1.</span>, <span class="number">0.</span>], high=[<span class="number">1.</span>, <span class="number">1.</span>])</span><br><span class="line">print(dist.prob([<span class="number">1.1</span>, <span class="number">-0.1</span>]))</span><br><span class="line">print(dist.log_prob([<span class="number">1.1</span>, <span class="number">-0.1</span>]))</span><br><span class="line"></span><br><span class="line">结果：</span><br><span class="line">tf.Tensor([<span class="number">0.</span> <span class="number">0.</span>], shape=(<span class="number">2</span>,), dtype=float32)</span><br><span class="line">tf.Tensor([-inf -inf], shape=(<span class="number">2</span>,), dtype=float32)</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;此篇博文用于记录和描述一些高斯分布的基本特性以及在tensorflow2.0中的不同之处。&lt;/p&gt;
    
    </summary>
    
      <category term="TensorFlow" scheme="http://StepNeverStop.github.io/categories/TensorFlow/"/>
    
    
      <category term="rl" scheme="http://StepNeverStop.github.io/tags/rl/"/>
    
      <category term="tf2" scheme="http://StepNeverStop.github.io/tags/tf2/"/>
    
  </entry>
  
  <entry>
    <title>学习gRPC过程点滴记录</title>
    <link href="http://StepNeverStop.github.io/learn-grpc.html"/>
    <id>http://StepNeverStop.github.io/learn-grpc.html</id>
    <published>2020-04-05T03:14:26.000Z</published>
    <updated>2020-04-26T03:15:16.605Z</updated>
    
    <content type="html"><![CDATA[<p><img src="./learn-grpc/gRPC.svg" alt=""></p><p>此博客用于记录学习gRPC(python)的过程。</p><a id="more"></a><h1 id="gRPC"><a href="#gRPC" class="headerlink" title="gRPC"></a>gRPC</h1><p>gRPC默认使用协议缓冲池(Protocol Buffers)进行服务器与客户端之间的交互，它是用于序列化结构化数据的成熟的谷歌开源机制。</p><p>大致流程是三步：</p><ul><li>Define a service in a <code>.proto</code> file.</li><li>Generate server and client code using the <code>protocol buffer compiler</code>.</li><li>Use the Python gRPC API to write a simple client and server for your service.</li></ul><h1 id="安装-gRPC"><a href="#安装-gRPC" class="headerlink" title="安装 gRPC"></a>安装 gRPC</h1><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> python -m pip install --upgrade pip</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> python -m pip install grpcio</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> python -m pip install grpcio-tools<span class="comment"># 这个用来编译proto文件</span></span></span><br></pre></td></tr></table></figure><h1 id="Protocal-Buffer-3"><a href="#Protocal-Buffer-3" class="headerlink" title="Protocal Buffer 3"></a>Protocal Buffer 3</h1><p>官方推荐使用<strong>proto3</strong>语法，因为它更简洁、强大，也支持更多语言。</p><p>一个简单的<code>.proto</code>文件示例：</p><figure class="highlight protobuf"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">syntax = <span class="string">"proto3"</span>;</span><br><span class="line"></span><br><span class="line">/* SearchRequest represents a search query, with pagination options to</span><br><span class="line"> * indicate which results to include in the response. */</span><br><span class="line"><span class="class"><span class="keyword">message</span> <span class="title">SearchRequest</span> </span>&#123;</span><br><span class="line">  <span class="built_in">string</span> query = <span class="number">1</span>;</span><br><span class="line">  <span class="built_in">int32</span> page_number = <span class="number">2</span>;</span><br><span class="line">  <span class="built_in">int32</span> result_per_page = <span class="number">3</span>;</span><br><span class="line">  <span class="keyword">repeated</span> <span class="built_in">int32</span> int_list=<span class="number">4</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><ul><li>第一行的<code>syntax</code>指定portobuf的语法版本，必须在第一行，不过不指定，默认为<code>proto2</code></li><li><code>message</code>用来定义客户端与服务器交互的数据结构，在上边例子中，<code>string query=1;</code>称为<code>field</code>，string/query/1分别为type/name/value</li><li>每一个<code>field</code>有一个独特的值，用于在二进制消息格式中标识<code>field</code>。这个值的范围是[1, 2^29-1(536870911)]，其中，[19000,19999]是禁用的，是为协议缓冲区实现保留的。1-15使用1个字节编码，16-2047使用2个字节，所以常出现的<code>filed</code>应该尽量给予小的值</li><li>注释是C/C++样式，使用<code>//</code>和<code>/*...*/</code></li><li><code>repeated</code>字段用于传输<strong>列表数据</strong>，比如上边的<code>int_list</code>，我就可以从客户端feed [1,2,3]，或者从客户端返回[1,2,3]</li></ul><p>proto与python数据类型对应表：</p><div class="table-container"><table><thead><tr><th style="text-align:center">proto</th><th style="text-align:center">Python</th></tr></thead><tbody><tr><td style="text-align:center">float/double</td><td style="text-align:center">float</td></tr><tr><td style="text-align:center">int32/sint32/sfixed32</td><td style="text-align:center">int</td></tr><tr><td style="text-align:center">int64/uint32/uint64/sint64/fixed32/fixed64/sfixed64</td><td style="text-align:center">int/long</td></tr><tr><td style="text-align:center">bool</td><td style="text-align:center">bool</td></tr><tr><td style="text-align:center">string</td><td style="text-align:center">str/unicode</td></tr><tr><td style="text-align:center">Bytes</td><td style="text-align:center">str</td></tr></tbody></table></div><p>默认值：</p><ul><li>strings, empty string</li><li>bytes, empty bytes</li><li>bools, false</li><li>numeric types, 0</li><li>enums, 默认值为第一个定义的枚举值，必须为0</li></ul><p>在<code>message</code>中使用枚举，可以这么定义：</p><figure class="highlight protobuf"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">message</span> <span class="title">Cooking</span> </span>&#123;</span><br><span class="line">    <span class="class"><span class="keyword">enum</span> <span class="title">VegeType</span> </span>&#123;</span><br><span class="line">        CAULIFLOWER = <span class="number">0</span>;</span><br><span class="line">        CUCUMBER = <span class="number">1</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">required</span> VegeType type = <span class="number">1</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>在python端可以用<strong>以下三种方式</strong>定义枚举字段：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">name_pb2.Cooking.CUCUMBER</span><br><span class="line">name_pb2.Cooking.VegeType.CUCUMBER</span><br><span class="line">name_pb2.Cooking.VegeTypeValue.Value(<span class="string">'CUCUMBER'</span>)</span><br></pre></td></tr></table></figure><p>对于客户端可RPC调用服务器的方式主要有四种，可以这样写：</p><figure class="highlight protobuf"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">syntax = <span class="string">"proto3"</span>;</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">service</span> <span class="title">RouteGuide</span> </span>&#123;</span><br><span class="line"><span class="comment">// 最简单方式，请求-&gt;响应</span></span><br><span class="line">  <span class="function"><span class="keyword">rpc</span> GetFeature(Point) <span class="keyword">returns</span> (Feature) &#123;&#125;</span></span><br><span class="line"><span class="function">// A server-to-client streaming RPC. 服务器流式响应</span></span><br><span class="line"><span class="function">  <span class="keyword">rpc</span> ListFeatures(Rectangle) <span class="keyword">returns</span> (stream Feature) &#123;&#125;</span></span><br><span class="line"><span class="function">// A client-to-server streaming RPC. 客户端流式请求</span></span><br><span class="line"><span class="function">  <span class="keyword">rpc</span> RecordRoute(stream Point) <span class="keyword">returns</span> (RouteSummary) &#123;&#125;</span></span><br><span class="line"><span class="function">  // A Bidirectional streaming RPC. 双向流式交互</span></span><br><span class="line"><span class="function">  <span class="keyword">rpc</span> RouteChat(stream RouteNote) <span class="keyword">returns</span> (stream RouteNote) &#123;&#125;</span></span><br><span class="line"><span class="function">&#125;</span></span><br></pre></td></tr></table></figure><p>重点在<code>stream</code>修饰符。</p><p>就拿<code>rpc GetFeature(Point) returns (Feature) {}</code>来说，它生成的服务器端函数接口应该是这样的：</p><p><code>def GetFeature(self, request, context):</code></p><p>其中，request即为客户端发送的<code>Point</code>消息类，context提供特定于rpc的信息，如超时限制，该方法返回给客户端<code>Feature</code>类。</p><h1 id="下载官方示例"><a href="#下载官方示例" class="headerlink" title="下载官方示例"></a>下载官方示例</h1><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> Clone the repository to get the example code:</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> git <span class="built_in">clone</span> -b v1.28.1 https://github.com/grpc/grpc</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> Navigate to the <span class="string">"hello, world"</span> Python example:</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> <span class="built_in">cd</span> grpc/examples/python/helloworld</span></span><br></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> server</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> python greeter_server.py</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> client</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> python greeter_client.py</span></span><br></pre></td></tr></table></figure><h1 id="生成服务器与客户端代码"><a href="#生成服务器与客户端代码" class="headerlink" title="生成服务器与客户端代码"></a>生成服务器与客户端代码</h1><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> python -m grpc_tools.protoc -I ../../protos --python_out=. --grpc_python_out=. ../../protos/[name].proto</span></span><br></pre></td></tr></table></figure><ul><li><code>--python_out</code>：指定编译生成处理 protobuf 相关的代码的路径</li><li><code>--grpc_python_out</code>：指定编译生成处理 grpc 相关的代码的路径</li><li><code>-I</code>：指定proto 文件的<strong>目录</strong>，然后再写文件路径</li></ul><p>这条命令会生成两个文件：</p><ul><li><code>name_pb2.py</code>包含我们定义的请求(request)、响应(response)类。</li><li><code>name_pb2_grpc.py</code>包含服务器(server)、客户端(client)类。<ul><li><code>&amp;Stub</code>类被客户端调用服务端RPC</li><li><code>&amp;Servicer</code>类定义实现服务端代码的接口</li><li>方法<code>add_&amp;Servicer_to_server</code>，把我们定义的服务器类添加到<code>grpc.Server</code></li></ul></li></ul><h1 id="编写服务器"><a href="#编写服务器" class="headerlink" title="编写服务器"></a>编写服务器</h1><p>主要步骤：</p><ul><li>实现服务端接口函数。Implementing the servicer interface generated from our service definition with functions that perform the actual “work” of the service.</li><li>开启服务器并监听客户端请求，产生响应。Running a gRPC server to listen for requests from clients and transmit responses.</li></ul><p>简而言之，就是重载<code>name_pb2_grpc.py</code>中<code>&amp;Servicer</code>类的所有RPC方法。比如:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">RouteGuideServicer</span><span class="params">(route_guide_pb2_grpc.RouteGuideServicer)</span>:</span></span><br><span class="line">    <span class="string">"""Provides methods that implement functionality of route guide server."""</span></span><br></pre></td></tr></table></figure><p>接下来就是开启服务器，监听客户端请求了。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">serve</span><span class="params">()</span>:</span></span><br><span class="line">    server = grpc.server(futures.ThreadPoolExecutor(max_workers=<span class="number">10</span>))</span><br><span class="line">    route_guide_pb2_grpc.add_RouteGuideServicer_to_server(RouteGuideServicer(), server)</span><br><span class="line">    server.add_insecure_port(<span class="string">'[::]:50051'</span>)</span><br><span class="line">    server.start()</span><br><span class="line">    server.wait_for_termination()</span><br></pre></td></tr></table></figure><h1 id="编写客户端"><a href="#编写客户端" class="headerlink" title="编写客户端"></a>编写客户端</h1><p>创建存根(stub)，从<code>name_pb2_grpc.py</code>中实例化<code>&amp;Stub</code>类。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">channel = grpc.insecure_channel(<span class="string">'localhost:50051'</span>)</span><br><span class="line">stub = route_guide_pb2_grpc.RouteGuideStub(channel)</span><br></pre></td></tr></table></figure><p>或者使用<code>with</code>上下文管理器：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">with</span> grpc.insecure_channel(<span class="string">'localhost:50051'</span>) <span class="keyword">as</span> channel:</span><br><span class="line">        stub = route_guide_pb2_grpc.RouteGuideStub(channel)</span><br></pre></td></tr></table></figure><p>使用存根调用服务器端的方法，可以有同步与异步两种。</p><p>同步：<code>feature = stub.GetFeature(point)</code></p><p>异步：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">feature_future = stub.GetFeature.future(point)</span><br><span class="line">feature = feature_future.result()</span><br></pre></td></tr></table></figure><h1 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h1><ul><li><a href="https://www.grpc.io/docs/quickstart/python/" rel="external nofollow" target="_blank">gRPC</a></li><li><a href="https://developers.google.com/protocol-buffers/docs/proto3" rel="external nofollow" target="_blank">https://developers.google.com/protocol-buffers/docs/proto3</a></li><li><a href="https://stackoverflow.com/questions/34407696/how-to-access-python-enums-in-protobufs" rel="external nofollow" target="_blank">How to access python enums in protobufs</a></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;img src=&quot;./learn-grpc/gRPC.svg&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;p&gt;此博客用于记录学习gRPC(python)的过程。&lt;/p&gt;
    
    </summary>
    
      <category term="Python" scheme="http://StepNeverStop.github.io/categories/Python/"/>
    
    
      <category term="python" scheme="http://StepNeverStop.github.io/tags/python/"/>
    
  </entry>
  
  <entry>
    <title>《Reinforcement Learning: An Introduction》阅读笔记</title>
    <link href="http://StepNeverStop.github.io/rl-an-introduction.html"/>
    <id>http://StepNeverStop.github.io/rl-an-introduction.html</id>
    <published>2020-04-02T13:29:07.000Z</published>
    <updated>2020-04-08T10:53:25.627Z</updated>
    
    <content type="html"><![CDATA[<p>学习RL至今(2020年04月02日21:30:19)，一直没有系统地看过这本被誉为RL界“圣经”的教科书，也没有对从该书中学到的知识点进行整理与记录，本文将记录重读《Reinforcement Learning: An Introduction》这本书时所学到的关键知识点和受到的启发。</p><a id="more"></a><h1 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h1><h2 id="Chapter-1-Introduction"><a href="#Chapter-1-Introduction" class="headerlink" title="Chapter 1 Introduction"></a>Chapter 1 Introduction</h2><h3 id="Reinforcement-Learning"><a href="#Reinforcement-Learning" class="headerlink" title="Reinforcement Learning"></a>Reinforcement Learning</h3><h3 id="Examples"><a href="#Examples" class="headerlink" title="Examples"></a>Examples</h3><h3 id="Elements-of-Reinforcement-Learning"><a href="#Elements-of-Reinforcement-Learning" class="headerlink" title="Elements of Reinforcement Learning"></a>Elements of Reinforcement Learning</h3><h3 id="Limitations-and-Scope"><a href="#Limitations-and-Scope" class="headerlink" title="Limitations and Scope"></a>Limitations and Scope</h3><h3 id="An-Extended-Example-Tic-Tac-Toe"><a href="#An-Extended-Example-Tic-Tac-Toe" class="headerlink" title="An Extended Example: Tic-Tac-Toe"></a>An Extended Example: Tic-Tac-Toe</h3><h3 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h3><h3 id="Early-History-of-Reinforcement-Learning"><a href="#Early-History-of-Reinforcement-Learning" class="headerlink" title="Early History of Reinforcement Learning"></a>Early History of Reinforcement Learning</h3><h1 id="Tabular-Solution-Methods"><a href="#Tabular-Solution-Methods" class="headerlink" title="Tabular Solution Methods"></a>Tabular Solution Methods</h1><h2 id="Chapter-2-Multi-armed-Bandits"><a href="#Chapter-2-Multi-armed-Bandits" class="headerlink" title="Chapter 2 Multi-armed Bandits"></a>Chapter 2 Multi-armed Bandits</h2><h3 id="A-k-armed-Bandit-Problem"><a href="#A-k-armed-Bandit-Problem" class="headerlink" title="A k-armed Bandit Problem"></a>A k-armed Bandit Problem</h3><h3 id="Action-value-Methods"><a href="#Action-value-Methods" class="headerlink" title="Action-value Methods"></a>Action-value Methods</h3><h3 id="The-10-armed-Testbed"><a href="#The-10-armed-Testbed" class="headerlink" title="The 10-armed Testbed"></a>The 10-armed Testbed</h3><h3 id="Incremental-Implementation"><a href="#Incremental-Implementation" class="headerlink" title="Incremental Implementation"></a>Incremental Implementation</h3><h3 id="Tracking-a-Nonstationary-Problem"><a href="#Tracking-a-Nonstationary-Problem" class="headerlink" title="Tracking a Nonstationary Problem"></a>Tracking a Nonstationary Problem</h3><h3 id="Optimistic-Initial-Values"><a href="#Optimistic-Initial-Values" class="headerlink" title="Optimistic Initial Values"></a>Optimistic Initial Values</h3><h3 id="Upper-Confidence-Bound-ActionSelection"><a href="#Upper-Confidence-Bound-ActionSelection" class="headerlink" title="Upper-Confidence-Bound ActionSelection"></a>Upper-Confidence-Bound ActionSelection</h3><h3 id="Gradient-Bandit-Algorithms"><a href="#Gradient-Bandit-Algorithms" class="headerlink" title="Gradient Bandit Algorithms"></a>Gradient Bandit Algorithms</h3><h3 id="Associative-Search-Contextual-Bandits"><a href="#Associative-Search-Contextual-Bandits" class="headerlink" title="Associative Search(Contextual Bandits)"></a>Associative Search(Contextual Bandits)</h3><h3 id="Summary-1"><a href="#Summary-1" class="headerlink" title="Summary"></a>Summary</h3><h2 id="Chapter-3-Finite-Markov-Decision-Processes"><a href="#Chapter-3-Finite-Markov-Decision-Processes" class="headerlink" title="Chapter 3 Finite Markov Decision Processes"></a>Chapter 3 Finite Markov Decision Processes</h2><h3 id="The-Agent-Environment-Interface"><a href="#The-Agent-Environment-Interface" class="headerlink" title="The Agent-Environment Interface"></a>The Agent-Environment Interface</h3><h3 id="Goals-and-Rewards"><a href="#Goals-and-Rewards" class="headerlink" title="Goals and Rewards"></a>Goals and Rewards</h3><h3 id="Returns-and-Episodes"><a href="#Returns-and-Episodes" class="headerlink" title="Returns and Episodes"></a>Returns and Episodes</h3><h3 id="Unified-Nation-for-Episodic-and-Continuing-Tasks"><a href="#Unified-Nation-for-Episodic-and-Continuing-Tasks" class="headerlink" title="Unified Nation for Episodic and Continuing Tasks"></a>Unified Nation for Episodic and Continuing Tasks</h3><h3 id="Policies-and-Value-Functions"><a href="#Policies-and-Value-Functions" class="headerlink" title="Policies and Value Functions"></a>Policies and Value Functions</h3><h3 id="Optimial-Policies-and-Optimal-Value-Functions"><a href="#Optimial-Policies-and-Optimal-Value-Functions" class="headerlink" title="Optimial Policies and Optimal Value Functions"></a>Optimial Policies and Optimal Value Functions</h3><h3 id="Optimality-and-Approximation"><a href="#Optimality-and-Approximation" class="headerlink" title="Optimality and Approximation"></a>Optimality and Approximation</h3><h3 id="Summary-2"><a href="#Summary-2" class="headerlink" title="Summary"></a>Summary</h3><h2 id="Chapter-4-Dynamic-Programming"><a href="#Chapter-4-Dynamic-Programming" class="headerlink" title="Chapter 4 Dynamic Programming"></a>Chapter 4 Dynamic Programming</h2><h3 id="Policy-Evaluation-Prediction"><a href="#Policy-Evaluation-Prediction" class="headerlink" title="Policy Evaluation(Prediction)"></a>Policy Evaluation(Prediction)</h3><h3 id="Policy-Improvement"><a href="#Policy-Improvement" class="headerlink" title="Policy Improvement"></a>Policy Improvement</h3><h3 id="Policy-Iteration"><a href="#Policy-Iteration" class="headerlink" title="Policy Iteration"></a>Policy Iteration</h3><h3 id="Value-Iteration"><a href="#Value-Iteration" class="headerlink" title="Value Iteration"></a>Value Iteration</h3><h3 id="Asynchronous-Dynamic-Programming"><a href="#Asynchronous-Dynamic-Programming" class="headerlink" title="Asynchronous Dynamic Programming"></a>Asynchronous Dynamic Programming</h3><h3 id="Generalized-Policy-Iteration"><a href="#Generalized-Policy-Iteration" class="headerlink" title="Generalized Policy Iteration"></a>Generalized Policy Iteration</h3><h3 id="Efficiency-of-Dynamic-Programming"><a href="#Efficiency-of-Dynamic-Programming" class="headerlink" title="Efficiency of Dynamic Programming"></a>Efficiency of Dynamic Programming</h3><h3 id="Summary-3"><a href="#Summary-3" class="headerlink" title="Summary"></a>Summary</h3><h2 id="Chapter-5-Monte-Carlo-Methods"><a href="#Chapter-5-Monte-Carlo-Methods" class="headerlink" title="Chapter 5 Monte Carlo Methods"></a>Chapter 5 Monte Carlo Methods</h2><h3 id="Monte-Carlo-Prediction"><a href="#Monte-Carlo-Prediction" class="headerlink" title="Monte Carlo Prediction"></a>Monte Carlo Prediction</h3><h3 id="Monte-Carlo-Estimation-of-Action-Values"><a href="#Monte-Carlo-Estimation-of-Action-Values" class="headerlink" title="Monte Carlo Estimation of Action Values"></a>Monte Carlo Estimation of Action Values</h3><h3 id="Monte-Carlo-Control"><a href="#Monte-Carlo-Control" class="headerlink" title="Monte Carlo Control"></a>Monte Carlo Control</h3><h3 id="Monte-Carlo-Control-without-Exploring-Starts"><a href="#Monte-Carlo-Control-without-Exploring-Starts" class="headerlink" title="Monte Carlo Control without Exploring Starts"></a>Monte Carlo Control without Exploring Starts</h3><h3 id="Off-policy-Prediction-via-Improtance-Sampling"><a href="#Off-policy-Prediction-via-Improtance-Sampling" class="headerlink" title="Off-policy Prediction via Improtance Sampling"></a>Off-policy Prediction via Improtance Sampling</h3><h3 id="Incremental-Implementation-1"><a href="#Incremental-Implementation-1" class="headerlink" title="Incremental Implementation"></a>Incremental Implementation</h3><h3 id="Off-policy-Monte-Carlo-Control"><a href="#Off-policy-Monte-Carlo-Control" class="headerlink" title="Off-policy Monte Carlo Control"></a>Off-policy Monte Carlo Control</h3><h3 id="Discounting-aware-Importance-Sampling"><a href="#Discounting-aware-Importance-Sampling" class="headerlink" title="Discounting-aware Importance Sampling"></a>Discounting-aware Importance Sampling</h3><h3 id="Per-decision-Importance-Sampling"><a href="#Per-decision-Importance-Sampling" class="headerlink" title="Per-decision Importance Sampling"></a>Per-decision Importance Sampling</h3><h3 id="Summary-4"><a href="#Summary-4" class="headerlink" title="Summary"></a>Summary</h3><h2 id="Chapter-6-Temporal-Difference-Learning"><a href="#Chapter-6-Temporal-Difference-Learning" class="headerlink" title="Chapter 6 Temporal-Difference Learning"></a>Chapter 6 Temporal-Difference Learning</h2><h3 id="TD-Prediction"><a href="#TD-Prediction" class="headerlink" title="TD Prediction"></a>TD Prediction</h3><h3 id="Advantages-of-TD-Prediction-Methods"><a href="#Advantages-of-TD-Prediction-Methods" class="headerlink" title="Advantages of TD Prediction Methods"></a>Advantages of TD Prediction Methods</h3><h3 id="Optimality-of-TD-0"><a href="#Optimality-of-TD-0" class="headerlink" title="Optimality of TD(0)"></a>Optimality of TD(0)</h3><h3 id="Sarsa-On-policy-TD-Control"><a href="#Sarsa-On-policy-TD-Control" class="headerlink" title="Sarsa: On-policy TD Control"></a>Sarsa: On-policy TD Control</h3><h3 id="Q-learning-Off-policy-TD-Control"><a href="#Q-learning-Off-policy-TD-Control" class="headerlink" title="Q-learning: Off-policy TD Control"></a>Q-learning: Off-policy TD Control</h3><h3 id="Expected-Sarsa"><a href="#Expected-Sarsa" class="headerlink" title="Expected Sarsa"></a>Expected Sarsa</h3><h3 id="Maximization-Bias-and-Double-Learning"><a href="#Maximization-Bias-and-Double-Learning" class="headerlink" title="Maximization Bias and Double Learning"></a>Maximization Bias and Double Learning</h3><h3 id="Games-Afterstates-and-Other-Special-Cases"><a href="#Games-Afterstates-and-Other-Special-Cases" class="headerlink" title="Games, Afterstates, and Other Special Cases"></a>Games, Afterstates, and Other Special Cases</h3><h3 id="Summary-5"><a href="#Summary-5" class="headerlink" title="Summary"></a>Summary</h3><h2 id="Chapter-7-n-step-Bootstrapping"><a href="#Chapter-7-n-step-Bootstrapping" class="headerlink" title="Chapter 7 n-step Bootstrapping"></a>Chapter 7 n-step Bootstrapping</h2><h3 id="n-step-TD-Prediction"><a href="#n-step-TD-Prediction" class="headerlink" title="n-step TD Prediction"></a>n-step TD Prediction</h3><h3 id="n-step-Sarsa"><a href="#n-step-Sarsa" class="headerlink" title="n-step Sarsa"></a>n-step Sarsa</h3><h3 id="n-step-Off-policy-Learning"><a href="#n-step-Off-policy-Learning" class="headerlink" title="n-step Off-policy Learning"></a>n-step Off-policy Learning</h3><h3 id="Per-decision-Methods-with-Control-Variates"><a href="#Per-decision-Methods-with-Control-Variates" class="headerlink" title="Per-decision Methods with Control Variates"></a>Per-decision Methods with Control Variates</h3><h3 id="Off-policy-Learning-Without-Importance-Sampling-The-n-step-Tree-Backup-Algorithm"><a href="#Off-policy-Learning-Without-Importance-Sampling-The-n-step-Tree-Backup-Algorithm" class="headerlink" title="Off-policy Learning Without Importance Sampling: The n-step Tree Backup Algorithm"></a>Off-policy Learning Without Importance Sampling: The n-step Tree Backup Algorithm</h3><h3 id="A-Unifying-Algorithm-n-step-Q-sigma"><a href="#A-Unifying-Algorithm-n-step-Q-sigma" class="headerlink" title="A Unifying Algorithm: n-step $Q(\sigma)$"></a>A Unifying Algorithm: n-step $Q(\sigma)$</h3><h3 id="Summary-6"><a href="#Summary-6" class="headerlink" title="Summary"></a>Summary</h3><h2 id="Chapter-8-Planning-and-Learning-with-Tabular-Methods"><a href="#Chapter-8-Planning-and-Learning-with-Tabular-Methods" class="headerlink" title="Chapter 8 Planning and Learning with Tabular Methods"></a>Chapter 8 Planning and Learning with Tabular Methods</h2><h3 id="Models-and-Planning"><a href="#Models-and-Planning" class="headerlink" title="Models and Planning"></a>Models and Planning</h3><h3 id="Dyna-Integrated-Planning-Acting-and-Learning"><a href="#Dyna-Integrated-Planning-Acting-and-Learning" class="headerlink" title="Dyna: Integrated Planning, Acting, and Learning"></a>Dyna: Integrated Planning, Acting, and Learning</h3><h3 id="When-the-Model-Is-Wrong"><a href="#When-the-Model-Is-Wrong" class="headerlink" title="When the Model Is Wrong"></a>When the Model Is Wrong</h3><h3 id="Prioritized-Sweeping"><a href="#Prioritized-Sweeping" class="headerlink" title="Prioritized Sweeping"></a>Prioritized Sweeping</h3><h3 id="Expected-vs-Sample-Updates"><a href="#Expected-vs-Sample-Updates" class="headerlink" title="Expected vs. Sample Updates"></a>Expected vs. Sample Updates</h3><h3 id="Trajectory-Sampling"><a href="#Trajectory-Sampling" class="headerlink" title="Trajectory Sampling"></a>Trajectory Sampling</h3><h3 id="Real-time-Dynamic-Programming"><a href="#Real-time-Dynamic-Programming" class="headerlink" title="Real-time Dynamic Programming"></a>Real-time Dynamic Programming</h3><h3 id="Planning-at-Decision-Time"><a href="#Planning-at-Decision-Time" class="headerlink" title="Planning at Decision Time"></a>Planning at Decision Time</h3><h3 id="Heuristic-Search"><a href="#Heuristic-Search" class="headerlink" title="Heuristic Search"></a>Heuristic Search</h3><h3 id="Rollout-Algorithms"><a href="#Rollout-Algorithms" class="headerlink" title="Rollout Algorithms"></a>Rollout Algorithms</h3><h3 id="Monte-Carlo-Tree-Search"><a href="#Monte-Carlo-Tree-Search" class="headerlink" title="Monte Carlo Tree Search"></a>Monte Carlo Tree Search</h3><h3 id="Summary-7"><a href="#Summary-7" class="headerlink" title="Summary"></a>Summary</h3><h1 id="Approximate-Solution-Methods"><a href="#Approximate-Solution-Methods" class="headerlink" title="Approximate Solution Methods"></a>Approximate Solution Methods</h1><h2 id="Chapter-9-On-policy-Prediction-with-Approximation"><a href="#Chapter-9-On-policy-Prediction-with-Approximation" class="headerlink" title="Chapter 9 On-policy Prediction with Approximation"></a>Chapter 9 On-policy Prediction with Approximation</h2><h3 id="Value-function-Approximation"><a href="#Value-function-Approximation" class="headerlink" title="Value-function Approximation"></a>Value-function Approximation</h3><h3 id="The-Prediction-Objective-VE"><a href="#The-Prediction-Objective-VE" class="headerlink" title="The Prediction Objective(VE)"></a>The Prediction Objective(VE)</h3><h3 id="Stochastic-gradient-and-Semi-gradient-Methods"><a href="#Stochastic-gradient-and-Semi-gradient-Methods" class="headerlink" title="Stochastic-gradient and Semi-gradient Methods"></a>Stochastic-gradient and Semi-gradient Methods</h3><h3 id="Linear-Methods"><a href="#Linear-Methods" class="headerlink" title="Linear Methods"></a>Linear Methods</h3><h3 id="Feature-Construction-for-Linear-Methods"><a href="#Feature-Construction-for-Linear-Methods" class="headerlink" title="Feature Construction for Linear Methods"></a>Feature Construction for Linear Methods</h3><h4 id="Polynomials"><a href="#Polynomials" class="headerlink" title="Polynomials"></a>Polynomials</h4><h4 id="Fourier-Basis"><a href="#Fourier-Basis" class="headerlink" title="Fourier Basis"></a>Fourier Basis</h4><h4 id="Coarse-Coding"><a href="#Coarse-Coding" class="headerlink" title="Coarse Coding"></a>Coarse Coding</h4><h4 id="Tile-Coding"><a href="#Tile-Coding" class="headerlink" title="Tile Coding"></a>Tile Coding</h4><h4 id="Radial-Basis-Functions"><a href="#Radial-Basis-Functions" class="headerlink" title="Radial Basis Functions"></a>Radial Basis Functions</h4><h3 id="Selecting-Step-size-Parameters-Manually"><a href="#Selecting-Step-size-Parameters-Manually" class="headerlink" title="Selecting Step-size Parameters Manually"></a>Selecting Step-size Parameters Manually</h3><h3 id="Nonlinear-Function-Approximation-Artiﬁcial-Neural-Networks"><a href="#Nonlinear-Function-Approximation-Artiﬁcial-Neural-Networks" class="headerlink" title="Nonlinear Function Approximation: Artiﬁcial Neural Networks"></a>Nonlinear Function Approximation: Artiﬁcial Neural Networks</h3><h3 id="Least-Squares-TD"><a href="#Least-Squares-TD" class="headerlink" title="Least-Squares TD"></a>Least-Squares TD</h3><h3 id="Memory-based-Function-Approximation"><a href="#Memory-based-Function-Approximation" class="headerlink" title="Memory-based Function Approximation"></a>Memory-based Function Approximation</h3><h3 id="Kernel-based-Function-Approximation"><a href="#Kernel-based-Function-Approximation" class="headerlink" title="Kernel-based Function Approximation"></a>Kernel-based Function Approximation</h3><h3 id="Looking-Deeper-at-On-policy-Learning-Interest-and-Emphasis"><a href="#Looking-Deeper-at-On-policy-Learning-Interest-and-Emphasis" class="headerlink" title="Looking Deeper at On-policy Learning: Interest and Emphasis"></a>Looking Deeper at On-policy Learning: Interest and Emphasis</h3><h3 id="Summary-8"><a href="#Summary-8" class="headerlink" title="Summary"></a>Summary</h3><h2 id="Chapter-10-On-policy-Control-with-Approximation"><a href="#Chapter-10-On-policy-Control-with-Approximation" class="headerlink" title="Chapter 10 On-policy Control with Approximation"></a>Chapter 10 On-policy Control with Approximation</h2><h3 id="Episodic-Semi-gradient-Control"><a href="#Episodic-Semi-gradient-Control" class="headerlink" title="Episodic Semi-gradient Control"></a>Episodic Semi-gradient Control</h3><h3 id="Semi-gradient-n-step-Sarsa"><a href="#Semi-gradient-n-step-Sarsa" class="headerlink" title="Semi-gradient n-step Sarsa"></a>Semi-gradient n-step Sarsa</h3><h3 id="Average-Reward-A-New-Problem-Setting-for-Continuing-Tasks"><a href="#Average-Reward-A-New-Problem-Setting-for-Continuing-Tasks" class="headerlink" title="Average Reward: A New Problem Setting for Continuing Tasks"></a>Average Reward: A New Problem Setting for Continuing Tasks</h3><h3 id="Deprecating-the-Discounted-Setting"><a href="#Deprecating-the-Discounted-Setting" class="headerlink" title="Deprecating the Discounted Setting"></a>Deprecating the Discounted Setting</h3><h3 id="Differential-Semi-gradient-n-step-Sarsa"><a href="#Differential-Semi-gradient-n-step-Sarsa" class="headerlink" title="Differential Semi-gradient n-step Sarsa"></a>Differential Semi-gradient n-step Sarsa</h3><h3 id="Summary-9"><a href="#Summary-9" class="headerlink" title="Summary"></a>Summary</h3><h2 id="Chapter-11-Off-policy-Methods-with-Approximation"><a href="#Chapter-11-Off-policy-Methods-with-Approximation" class="headerlink" title="Chapter 11 Off-policy Methods with Approximation"></a>Chapter 11 Off-policy Methods with Approximation</h2><h3 id="Semi-gradient-Methods"><a href="#Semi-gradient-Methods" class="headerlink" title="Semi-gradient Methods"></a>Semi-gradient Methods</h3><h3 id="Examples-of-Off-policy-Divergence"><a href="#Examples-of-Off-policy-Divergence" class="headerlink" title="Examples of Off-policy Divergence"></a>Examples of Off-policy Divergence</h3><h3 id="The-Deadly-Triad"><a href="#The-Deadly-Triad" class="headerlink" title="The Deadly Triad"></a>The Deadly Triad</h3><h3 id="Linear-Value-function-Geometry"><a href="#Linear-Value-function-Geometry" class="headerlink" title="Linear Value-function Geometry"></a>Linear Value-function Geometry</h3><h3 id="Gradient-Descent-in-the-Bellman-Error"><a href="#Gradient-Descent-in-the-Bellman-Error" class="headerlink" title="Gradient Descent in the Bellman Error"></a>Gradient Descent in the Bellman Error</h3><h3 id="The-Bellman-Error-is-Not-Learnable"><a href="#The-Bellman-Error-is-Not-Learnable" class="headerlink" title="The Bellman Error is Not Learnable"></a>The Bellman Error is Not Learnable</h3><h3 id="Gradient-TD-Methods"><a href="#Gradient-TD-Methods" class="headerlink" title="Gradient-TD Methods"></a>Gradient-TD Methods</h3><h3 id="Emphatic-TD-Methods"><a href="#Emphatic-TD-Methods" class="headerlink" title="Emphatic-TD Methods"></a>Emphatic-TD Methods</h3><h3 id="Reducing-Variance"><a href="#Reducing-Variance" class="headerlink" title="Reducing Variance"></a>Reducing Variance</h3><h3 id="Summary-10"><a href="#Summary-10" class="headerlink" title="Summary"></a>Summary</h3><h2 id="Chapter-12-Eligibility-Traces"><a href="#Chapter-12-Eligibility-Traces" class="headerlink" title="Chapter 12 Eligibility Traces"></a>Chapter 12 Eligibility Traces</h2><h3 id="The-lambda-return"><a href="#The-lambda-return" class="headerlink" title="The $\lambda$-return"></a>The $\lambda$-return</h3><h3 id="TD-lambda"><a href="#TD-lambda" class="headerlink" title="TD($\lambda$)"></a>TD($\lambda$)</h3><h3 id="n-step-Truncated-lambda-return-Methods"><a href="#n-step-Truncated-lambda-return-Methods" class="headerlink" title="n-step Truncated $\lambda$-return Methods"></a>n-step Truncated $\lambda$-return Methods</h3><h3 id="Redoing-Updates-Online-lambda-return-Algorithm"><a href="#Redoing-Updates-Online-lambda-return-Algorithm" class="headerlink" title="Redoing Updates: Online $\lambda$-return Algorithm"></a>Redoing Updates: Online $\lambda$-return Algorithm</h3><h3 id="True-Online-TD-lambda"><a href="#True-Online-TD-lambda" class="headerlink" title="True Online TD($\lambda$)"></a>True Online TD($\lambda$)</h3><h3 id="Dutch-Traces-in-Monte-Carlo-Learning"><a href="#Dutch-Traces-in-Monte-Carlo-Learning" class="headerlink" title="Dutch Traces in Monte Carlo Learning"></a>Dutch Traces in Monte Carlo Learning</h3><h3 id="Sarsa-lambda"><a href="#Sarsa-lambda" class="headerlink" title="Sarsa($\lambda$)"></a>Sarsa($\lambda$)</h3><h3 id="Variable-lambda-and-gamma"><a href="#Variable-lambda-and-gamma" class="headerlink" title="Variable $\lambda$ and $\gamma$"></a>Variable $\lambda$ and $\gamma$</h3><h3 id="Off-policy-Traces-with-Control-Variates"><a href="#Off-policy-Traces-with-Control-Variates" class="headerlink" title="Off-policy Traces with Control Variates"></a>Off-policy Traces with Control Variates</h3><h3 id="Watkins’s-Q-lambda-to-Tree-Backup-lambda"><a href="#Watkins’s-Q-lambda-to-Tree-Backup-lambda" class="headerlink" title="Watkins’s Q($\lambda$) to Tree-Backup( $\lambda$)"></a>Watkins’s Q($\lambda$) to Tree-Backup( $\lambda$)</h3><h3 id="Stable-Off-policy-Methods-with-Traces"><a href="#Stable-Off-policy-Methods-with-Traces" class="headerlink" title="Stable Off-policy Methods with Traces"></a>Stable Off-policy Methods with Traces</h3><h3 id="Implementation-Issues"><a href="#Implementation-Issues" class="headerlink" title="Implementation Issues"></a>Implementation Issues</h3><h3 id="Conclusions"><a href="#Conclusions" class="headerlink" title="Conclusions"></a>Conclusions</h3><h2 id="Chapter-13-Policy-Gradient-Methods"><a href="#Chapter-13-Policy-Gradient-Methods" class="headerlink" title="Chapter 13 Policy Gradient Methods"></a>Chapter 13 Policy Gradient Methods</h2><h3 id="Policy-Approximation-and-its-Advantages"><a href="#Policy-Approximation-and-its-Advantages" class="headerlink" title="Policy Approximation and its Advantages"></a>Policy Approximation and its Advantages</h3><h3 id="The-Policy-Gradient-Theorem"><a href="#The-Policy-Gradient-Theorem" class="headerlink" title="The Policy Gradient Theorem"></a>The Policy Gradient Theorem</h3><h3 id="REINFORCE-Monte-Carlo-Policy-Gradient"><a href="#REINFORCE-Monte-Carlo-Policy-Gradient" class="headerlink" title="REINFORCE: Monte Carlo Policy Gradient"></a>REINFORCE: Monte Carlo Policy Gradient</h3><h3 id="REINFORCE-with-Baseline"><a href="#REINFORCE-with-Baseline" class="headerlink" title="REINFORCE with Baseline"></a>REINFORCE with Baseline</h3><h3 id="Actor–Critic-Methods"><a href="#Actor–Critic-Methods" class="headerlink" title="Actor–Critic Methods"></a>Actor–Critic Methods</h3><h3 id="Policy-Gradient-for-Continuing-Problems"><a href="#Policy-Gradient-for-Continuing-Problems" class="headerlink" title="Policy Gradient for Continuing Problems"></a>Policy Gradient for Continuing Problems</h3><h3 id="Policy-Parameterization-for-Continuous-Actions"><a href="#Policy-Parameterization-for-Continuous-Actions" class="headerlink" title="Policy Parameterization for Continuous Actions"></a>Policy Parameterization for Continuous Actions</h3><h3 id="Summary-11"><a href="#Summary-11" class="headerlink" title="Summary"></a>Summary</h3><h1 id="Looking-Deeper"><a href="#Looking-Deeper" class="headerlink" title="Looking Deeper"></a>Looking Deeper</h1><h2 id="Chapter-14-Psychology"><a href="#Chapter-14-Psychology" class="headerlink" title="Chapter 14 Psychology"></a>Chapter 14 Psychology</h2><h3 id="Prediction-and-Control"><a href="#Prediction-and-Control" class="headerlink" title="Prediction and Control"></a>Prediction and Control</h3><h3 id="Classical-Conditioning"><a href="#Classical-Conditioning" class="headerlink" title="Classical Conditioning"></a>Classical Conditioning</h3><h4 id="Blocking-and-Higher-order-Conditioning"><a href="#Blocking-and-Higher-order-Conditioning" class="headerlink" title="Blocking and Higher-order Conditioning"></a>Blocking and Higher-order Conditioning</h4><h4 id="The-Rescorla–Wagner-Model"><a href="#The-Rescorla–Wagner-Model" class="headerlink" title="The Rescorla–Wagner Model"></a>The Rescorla–Wagner Model</h4><h4 id="The-TD-Model"><a href="#The-TD-Model" class="headerlink" title="The TD Model"></a>The TD Model</h4><h4 id="TD-Model-Simulations"><a href="#TD-Model-Simulations" class="headerlink" title="TD Model Simulations"></a>TD Model Simulations</h4><h3 id="Instrumental-Conditioning"><a href="#Instrumental-Conditioning" class="headerlink" title="Instrumental Conditioning"></a>Instrumental Conditioning</h3><h3 id="Delayed-Reinforcement"><a href="#Delayed-Reinforcement" class="headerlink" title="Delayed Reinforcement"></a>Delayed Reinforcement</h3><h3 id="Cognitive-Maps"><a href="#Cognitive-Maps" class="headerlink" title="Cognitive Maps"></a>Cognitive Maps</h3><h3 id="Habitual-and-Goal-directed-Behavior"><a href="#Habitual-and-Goal-directed-Behavior" class="headerlink" title="Habitual and Goal-directed Behavior"></a>Habitual and Goal-directed Behavior</h3><h3 id="Summary-12"><a href="#Summary-12" class="headerlink" title="Summary"></a>Summary</h3><h2 id="Chapter-15-Neuroscience"><a href="#Chapter-15-Neuroscience" class="headerlink" title="Chapter 15 Neuroscience"></a>Chapter 15 Neuroscience</h2><h3 id="Neuroscience-Basics"><a href="#Neuroscience-Basics" class="headerlink" title="Neuroscience Basics"></a>Neuroscience Basics</h3><h3 id="Reward-Signals-Reinforcement-Signals-Values-and-Prediction-Errors"><a href="#Reward-Signals-Reinforcement-Signals-Values-and-Prediction-Errors" class="headerlink" title="Reward Signals, Reinforcement Signals, Values, and Prediction Errors"></a>Reward Signals, Reinforcement Signals, Values, and Prediction Errors</h3><h3 id="The-Reward-Prediction-Error-Hypothesis"><a href="#The-Reward-Prediction-Error-Hypothesis" class="headerlink" title="The Reward Prediction Error Hypothesis"></a>The Reward Prediction Error Hypothesis</h3><h3 id="Dopamine"><a href="#Dopamine" class="headerlink" title="Dopamine"></a>Dopamine</h3><h3 id="Experimental-Support-for-the-Reward-Prediction-Error-Hypothesis"><a href="#Experimental-Support-for-the-Reward-Prediction-Error-Hypothesis" class="headerlink" title="Experimental Support for the Reward Prediction Error Hypothesis"></a>Experimental Support for the Reward Prediction Error Hypothesis</h3><h3 id="TD-Error-Dopamine-Correspondence"><a href="#TD-Error-Dopamine-Correspondence" class="headerlink" title="TD Error/Dopamine Correspondence"></a>TD Error/Dopamine Correspondence</h3><h3 id="Neural-Actor–Critic"><a href="#Neural-Actor–Critic" class="headerlink" title="Neural Actor–Critic"></a>Neural Actor–Critic</h3><h3 id="Actor-and-Critic-Learning-Rules"><a href="#Actor-and-Critic-Learning-Rules" class="headerlink" title="Actor and Critic Learning Rules"></a>Actor and Critic Learning Rules</h3><h3 id="Hedonistic-Neurons"><a href="#Hedonistic-Neurons" class="headerlink" title="Hedonistic Neurons"></a>Hedonistic Neurons</h3><h3 id="Collective-Reinforcement-Learning"><a href="#Collective-Reinforcement-Learning" class="headerlink" title="Collective Reinforcement Learning"></a>Collective Reinforcement Learning</h3><h3 id="Model-based-Methods-in-the-Brain"><a href="#Model-based-Methods-in-the-Brain" class="headerlink" title="Model-based Methods in the Brain"></a>Model-based Methods in the Brain</h3><h3 id="Addiction"><a href="#Addiction" class="headerlink" title="Addiction"></a>Addiction</h3><h3 id="Summary-13"><a href="#Summary-13" class="headerlink" title="Summary"></a>Summary</h3><h2 id="Chapter-16-Applications-and-Case-Studies"><a href="#Chapter-16-Applications-and-Case-Studies" class="headerlink" title="Chapter 16 Applications and Case Studies"></a>Chapter 16 Applications and Case Studies</h2><h3 id="TD-Gammon"><a href="#TD-Gammon" class="headerlink" title="TD-Gammon"></a>TD-Gammon</h3><h3 id="Samuel’s-Checkers-Player"><a href="#Samuel’s-Checkers-Player" class="headerlink" title="Samuel’s Checkers Player"></a>Samuel’s Checkers Player</h3><h3 id="Watson’s-Daily-Double-Wagering"><a href="#Watson’s-Daily-Double-Wagering" class="headerlink" title="Watson’s Daily-Double Wagering"></a>Watson’s Daily-Double Wagering</h3><h3 id="Optimizing-Memory-Control"><a href="#Optimizing-Memory-Control" class="headerlink" title="Optimizing Memory Control"></a>Optimizing Memory Control</h3><h3 id="Human-level-Video-Game-Play"><a href="#Human-level-Video-Game-Play" class="headerlink" title="Human-level Video Game Play"></a>Human-level Video Game Play</h3><h3 id="Mastering-the-Game-of-Go"><a href="#Mastering-the-Game-of-Go" class="headerlink" title="Mastering the Game of Go"></a>Mastering the Game of Go</h3><h4 id="AlphaGo"><a href="#AlphaGo" class="headerlink" title="AlphaGo"></a>AlphaGo</h4><h4 id="AlphaGo-Zero"><a href="#AlphaGo-Zero" class="headerlink" title="AlphaGo Zero"></a>AlphaGo Zero</h4><h3 id="Personalized-Web-Services"><a href="#Personalized-Web-Services" class="headerlink" title="Personalized Web Services"></a>Personalized Web Services</h3><h3 id="Thermal-Soaring"><a href="#Thermal-Soaring" class="headerlink" title="Thermal Soaring"></a>Thermal Soaring</h3><h2 id="Chapter-17-Frontiers"><a href="#Chapter-17-Frontiers" class="headerlink" title="Chapter 17 Frontiers"></a>Chapter 17 Frontiers</h2><h3 id="General-Value-Functions-and-Auxiliary-Tasks"><a href="#General-Value-Functions-and-Auxiliary-Tasks" class="headerlink" title="General Value Functions and Auxiliary Tasks"></a>General Value Functions and Auxiliary Tasks</h3><h3 id="Temporal-Abstraction-via-Options"><a href="#Temporal-Abstraction-via-Options" class="headerlink" title="Temporal Abstraction via Options"></a>Temporal Abstraction via Options</h3><h3 id="Observations-and-State"><a href="#Observations-and-State" class="headerlink" title="Observations and State"></a>Observations and State</h3><h3 id="Designing-Reward-Signals"><a href="#Designing-Reward-Signals" class="headerlink" title="Designing Reward Signals"></a>Designing Reward Signals</h3><h3 id="Remaining-Issues"><a href="#Remaining-Issues" class="headerlink" title="Remaining Issues"></a>Remaining Issues</h3><h3 id="The-Future-of-Artiﬁcial-Intelligence"><a href="#The-Future-of-Artiﬁcial-Intelligence" class="headerlink" title="The Future of Artiﬁcial Intelligence"></a>The Future of Artiﬁcial Intelligence</h3>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;学习RL至今(2020年04月02日21:30:19)，一直没有系统地看过这本被誉为RL界“圣经”的教科书，也没有对从该书中学到的知识点进行整理与记录，本文将记录重读《Reinforcement Learning: An Introduction》这本书时所学到的关键知识点和受到的启发。&lt;/p&gt;
    
    </summary>
    
      <category term="ReinforcementLearning" scheme="http://StepNeverStop.github.io/categories/ReinforcementLearning/"/>
    
    
      <category term="rl" scheme="http://StepNeverStop.github.io/tags/rl/"/>
    
  </entry>
  
  <entry>
    <title>Keeping Your Distance: Solving Sparse Reward Tasks Using Self-Balancing Shaped Rewards</title>
    <link href="http://StepNeverStop.github.io/keeping-your-distance-solving-sparse-reward-tasks.html"/>
    <id>http://StepNeverStop.github.io/keeping-your-distance-solving-sparse-reward-tasks.html</id>
    <published>2020-03-30T10:09:45.000Z</published>
    <updated>2020-04-09T03:46:55.796Z</updated>
    
    <content type="html"><![CDATA[<p>这篇论文介绍了一个简单有效的model-free方法——<strong>Sibling Rivalry</strong>(同胞对抗？)，用于解决稀疏奖励问题。该方法特定于“以达到某个目标状态(goal-oriented)”为任务的问题，并且从塑性的距离目标相关奖励(distance-to-goal rewards)中学习。</p><p>推荐：</p><ul><li>self-balancing 奖励机制</li><li>基于奖励函数的创新，比较有趣</li></ul><a id="more"></a><h1 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h1><p>论文地址：<a href="http://arxiv.org/abs/1911.01417" rel="external nofollow" target="_blank">http://arxiv.org/abs/1911.01417.pdf</a></p><p>这篇文章解决的问题点是：</p><ul><li>由朴素的距离目标相关的奖励(naive distance-to-goal reward shaping)引起的动态学习在局部最优点稳定的问题</li></ul><p>这篇文章方法的优点是：</p><ul><li>增强目标但不需额外奖励工程设计，也不需领域专家知识</li><li>可以收敛至原始的稀疏奖励目标</li></ul><p>文章发表在NeurIPS 2019上。</p><p>这篇论文提出的Sibling Rivalry(SR)方法结合了塑性奖励的可学习性与稀疏奖励的通用性。它主要解决面向目标(goal-oriented)的任务，是对距离奖励函数的改进，实现了奖励函数的自平衡(self-balancing，自动退化至纯稀疏奖励形式)，它的特点是：</p><ul><li>model-free</li><li>应用在on-policy算法上</li><li>应用在<strong><em>目标状态已知</em></strong>，且<strong><em>以距离为奖励函数导向</em></strong>的任务中</li><li><strong>动态</strong>奖励塑性</li><li>保持稀疏奖励任务的原有最优策略</li><li>跳出局部最优解，寻找全局最优</li><li>可以与分层RL相结合（有实验佐证）</li></ul><h1 id="文中精要"><a href="#文中精要" class="headerlink" title="文中精要"></a>文中精要</h1><p>文中shaped rewards一时不知道该如何做翻译，姑且就称之为“塑性奖励”好了，意为为解决某一特定问题而精心设计的奖励函数机制。</p><p>Reward shaping是一种修改奖励信号的技术，比如，它可以用于重新标注失败的经验序列，并从其中筛选出可促进任务完成的经验序列进行学习。然而，这种技术是否可以提升任务性能严重取决于塑性奖励的精心设计。这种塑性奖励有时可以解决稀疏奖励问题，但是具有两个显著特点：</p><ol><li><p>需要精心的工程设计，也可以理解为需要对环境或任务的先验知识</p><blockquote><p>requires careful engineering</p></blockquote></li><li><p>往往只适用于特定任务，特定问题，比如同样的避障问题，载具类型不同，可能就不适用</p><blockquote><p>is problem specific</p></blockquote></li></ol><p>对于现实世界的RL问题，需要手动设计一个与任务契合/对齐的奖励机制，一个好的奖励函数往往比算法的选择更加重要。但是现实世界问题非常复杂，细粒度的奖励函数也十分难以设计，往往会具有“捡了芝麻丢了西瓜”的特点。比如，在避障问题中，如果针对墙壁这种障碍物设置奖励函数，策略在学习过程中会对墙壁这种实体过拟合，导致当遇到一种新的障碍物时，策略无法适用，还不如简单的稀疏奖励（完成即获得奖励，反之则无），让智能体自己从环境中学习哪些具有障碍物共有或特有特性。</p><p>还有一个很严重的问题是，复杂的奖励机制容易使策略陷入局部最优，比如说达到状态A可以获得奖励+10，而达到状态B可以获得奖励+1，那么如果状态A十分难以到达，策略往往会收敛在状态B，或者其他相似的状态附近中去，造成策略在这些局部最优解附近稳定下来。</p><p>稀疏奖励往往不存在这种局部最优，或者考虑不周的问题。设计一个合适的稀疏奖励函数很简单，也很直接，但是从这种奖励函数中学习需要大量的时间，而且甚至学习不出来任何东西，<strong>通常需要额外的启发式探索机制去帮助智能体发现这个稀疏的正奖励</strong>。</p><h1 id="方法"><a href="#方法" class="headerlink" title="方法"></a>方法</h1><p>纯稀疏奖励的形式是这样的：</p><script type="math/tex; mode=display">r(s, g)=\left\{\begin{array}{ll}1, & d(s, g) \leq \delta \\0, & \text { otherwise }\end{array}\right.</script><p>其中，$s$代表当前状态，$g$代表目标状态。$d$是一个函数用来判断当前状态与目标状态的距离度量，例如$L_1$或$L_2$距离，这个距离可以用来表示任务的完成度，即距离越短，任务完成度越高。$\delta$代表目标点的半径，也就是说，状态不必完全等于目标状态，只需要距离度量小于一定范围$\delta$即可，这个是很常用的做法，比如Unity ML-Agents的示例环境——RollerBall，也是使用这样的设置。</p><p>这种纯稀疏奖励的设置很难解决，往往需要很大的探索才能获取到寥寥无几的正奖励样本轨迹，给智能体的学习带来了很大的困难。</p><p>其实可以对上述奖励进行修改，使得其可以按照距目标状态的距离给出不同的奖励，引导智能体朝向目标状态移动。它通常为下边这种形式：</p><script type="math/tex; mode=display">\begin{array}{ll}\tilde{r}(s, g)= & \left\{\begin{array}{ll}1, & d(s, g) \leq \delta \\-d(s, g), & \text { otherwise }\end{array}\right.\end{array}</script><p>这种奖励形式很直观，如果没有达到目标状态，则一直是负奖励，且距离越远则负奖励越大，这样将会引导智能体朝向目标状态移动。针对一些比较简单的稀疏奖励环境，这种形式的奖励设计可以带来算法性能的提升，而且往往可以解决稀疏奖励问题。但是，这种基于距离的塑性奖励（Distance-based shaped rewards）很容易陷入局部最优点，并且在局部最优点附近稳定下来，得不到进一步地策略提升。为什么这么说呢？就比如gym的MountainCar场景，智能体需要先倒退再前进才能达到目标点，也就是说要先经历负奖励的增大过程，再经历负奖励的减小过程，才能最终完成目标，从初始状态直接朝向目标点移动是不能把车子开到山顶上去的。使用这种形式的奖励将会使得智能体抵达半山腰，却永远都触及不到目标点。还有一些U-shape的路径问题，在U型的两端设置起始点与目标点，使用这种奖励机制将会使得智能体在直线朝向目标点移动时偏离轨道，永远学不会以U型的方式完成目标。</p><p>使用上述奖励形式的问题经验存在各种各样的局部最优点，而且这些局部最优点与状态空间结构（state space structure）、转移动态（transition dynamics）和环境的其他特性都有关系。</p><p>那么有没有一种对奖励进行塑性的方式可以避免局部最优点的影响呢？确实是有的，我们再对上述奖励函数做如下改进：</p><script type="math/tex; mode=display">r^{\prime}(s, g, \bar{g})=\left\{\begin{array}{ll}1, & d(s, g) \leq \delta \\\min [0,-d(s, g)+d(s, \bar{g})], & \text { otherwise }\end{array}\right.</script><p>与上式不同的是，这里多了一个符号——$\bar g$，它用来表示局部最优点的状态。论文中给出了这样一个例子：</p><p><img src="./keeping-your-distance-solving-sparse-reward-tasks/motivating-example.png" alt=""></p><p>最左边的图是一个简单的环境，智能体的目标是到达绿色的Goal点，如果使用前文提到的$\tilde{r}$，则会存在局部最优点，也就是图中的Local Optimum。为什么会存在这个局部最优点呢？可以设想一下，假设智能体从左下角出发，那么它在上方路径和下方路径各走一步时，明显下方路径的负奖励衰减的更快，所以智能体通常会选择走下方路径，最终收敛到与Goal隔岸相望的Local Optimum。$\tilde{r}$的奖励函数图像轮廓如上面中间图像红色线条所示，图中横坐标为坐标点，纵坐标为奖励值的高低。绿色光点为智能体在轨迹终态的分布，可以看出，使用$\tilde{r}$作为奖励函数时，在这个环境中奖励图像存在一个小的山峰，相比于全局最优点，智能体更易达到局部最优点，而且会在局部最优附近稳定下来，导致无法产生全局最优策略。</p><p>最右侧的图是使用了增强后的距离奖励函数$r^{\prime}$所描绘的奖励图像轮廓，可以看出，通过将局部最优点的山峰构造成低谷，即可抵消局部最优点的影响。绿色点的分布也彰显了增强奖励函数后，智能体可以收敛到全局最优策略。为什么像$r^{\prime}$一样构造奖励函数即可解决局部最优点的影响呢？可以想象一下，将三角形的原理应用上去，当智能体选择上方路径时，两距离相减值逐渐趋于正值，也就是奖励逐渐增大，而选择下方路径时， 两距离相减横为负奖励，这种特性引导着智能体向逐渐增大的正奖励的方向靠拢。</p><p>虽然像$r^{\prime}$这种增强距离奖励的形式可以解决稀疏奖励问题，并跳出局部最优点，但是其通常具有如下几个缺点：</p><ol><li>需要领域专家知识分析局部最优点；</li><li>环境或任务复杂时，存在多种局部最优点，难以确定合适的奖励函数；</li><li>一不小心便会弄巧成拙，还会引入新的局部最优点。</li></ol><h2 id="同胞对抗——Sibling-Rivalry"><a href="#同胞对抗——Sibling-Rivalry" class="headerlink" title="同胞对抗——Sibling Rivalry"></a>同胞对抗——Sibling Rivalry</h2><p>作者十分中意这种跳出局部最优点的方式，所以，作者设想需要一个新的奖励机制，它应当满足：</p><ol><li>可以解决稀疏奖励问题</li><li>不需要专家及领域知识对环境进行全面的分析与判断</li><li>动态评估局部最优点，自适应调整奖励函数</li></ol><p>这有点像静态图与动态图的区别，$r^{\prime}$就好像是静态图，需要专家先把图构好，再解决问题，而同胞对抗（Sibling Rivalry， SR）的思想就好像是动态图，一边执行一边应对潜在的局部最优点。</p><p>先来说一下SR的思想：</p><ol><li><p>每次采样两条轨迹，同样的起始点与目标点；</p></li><li><p>距离目标近的为$\tau^c$，远的为$\tau^f$;</p></li><li><p>互相认为对方的终态$s_T$为局部最优点（假想敌），构造奖励函数，计算轨迹每一时间步的奖励。</p></li></ol><p>如下图所示，每次都roll out两条轨迹，这两条轨迹即称为同胞，然后互相指认对方的终态$S_T^c$和$S_T^f$为局部最优点，促使智能体不选择对方的路径，即为对抗。注意，这样的方式确实有可能使得策略在远离局部最优点的同时，也偏离潜在的正确路径，因为智能体并不真正了解什么样的路径是比较好或者比较坏的。</p><p><img src="./keeping-your-distance-solving-sparse-reward-tasks/sr-example.png" alt=""></p><p>这种互相对抗的方式增加了智能体的探索能力，因为当智能体的策略不是最优时，它会趋向于选择各种不同的路径，以使得策略可以避开尽可能多的局部最优点。</p><p>基于两条同胞轨迹$\tau^{c}$和$\tau^{f}$，如何构造它们的奖励函数呢？其实也很简单，像$r^{\prime}$一样，将其中的局部最优点$\bar g$更换一下即可：</p><script type="math/tex; mode=display">r_{\tau^{f}}^{\prime}=r^{\prime}\left(s_{T}^{f}, g, s_{T}^{c}\right) \quad \& \quad r_{\tau^{c}}^{\prime}=r^{\prime}\left(s_{T}^{c}, g, s_{T}^{f}\right)</script><p>作者将这种奖励机制称为<strong>Self-balancing reward</strong>，可以翻译为自平衡/自适应奖励。什么意思呢？可以想想一下，如果策略已经接近全局最优了，那么$g \approx s_{T}^{f} \approx s_{T}^{c}$，此时的奖励从$r^{\prime}_{\tau^{f}}$和$r^{\prime}_{\tau^{c}}$逐渐退化至纯稀疏奖励$r$。为什么呢？因为此时的$-d(s, g)+d(s, \bar{g})$几乎为0呀！</p><p>这样自动退化的奖励机制有什么好处呢？会<strong>使得智能体最终的最优策略与使用纯稀疏奖励时一致</strong>，大道至简，复杂的奖励函数未必可以引导智能体习得期望的行为，然而，纯稀疏奖励虽然难以学习，但是其产生的智能体行为往往是最直接最符合设计者的期望的。</p><p>有了轨迹经验，也根据各自的奖励函数生成了奖励，那么该如何进行优化呢？既然取得了两条同胞轨迹，两条轨迹都要拿来训练么？我们结合伪代码进行分析。</p><h1 id="伪代码"><a href="#伪代码" class="headerlink" title="伪代码"></a>伪代码</h1><p><img src="./keeping-your-distance-solving-sparse-reward-tasks/pseudo.png" alt=""></p><p>解析：</p><ul><li>$\rho(s_0, g)$为一个分布，用于采样任务的初始状态和目标状态，论文中的实验都是将智能体的起止点和目标点固定在一个小范围内，至于SR是否可以用在大范围随机的起始、目标点场景中，还未可知；</li><li>$m$是一个函数映射，用于将智能体状态空间$S$映射至目标状态空间$G$，$m(s): S \rightarrow G$；</li><li>$d$是一个距离度量函数，通常为$L_1,L_2$度量；</li><li>$\delta$是判定目标完成的容错半径；</li><li>注意，Critic网络V的输入不仅包括状态$s$，目标$g$，也包括局部最优点（同胞轨迹的终态）$\bar g$；</li><li>作者引入了一个“包容性阈值”$\epsilon$，这个值用来控制对使用距离目标点较近的轨迹$\tau^c$来训练的容忍度，也用来平衡探索与利用，当$\epsilon\uparrow$，利用增加，当$\epsilon\downarrow$，探索增加（学习远距离经验）；</li><li>SR是先采样轨迹$s,a,s,a,…$再计算每一步的奖励的，采样时不包括$r$；</li><li>每次rollout两条轨迹——$\tau^a$和$\tau^b$，距离目标点$g$比较近的终态轨迹标记为$\tau^c$，远的标记为$\tau^{f}$；</li><li>优化模型时，何时用$\tau^f$？<ul><li>任何时候</li><li>猜想：距离远的不太可能是局部最优点，因为距离越远，负奖励越大，算法不会稳定收敛在距离目标点很远的局部最优处，会想办法跳出来</li></ul></li><li>优化模型时，何时用$\tau^c$?<ul><li>1，当两条同胞轨迹的终态距离小于$\epsilon$时；2，当$\tau^c$的终态已足以完成目标时</li><li>猜想：当两条轨迹的终态距离小于一定阈值时，认为轨迹质量差不多，都可以用来更新</li><li>距离目标点$g$近的轨迹已经足够近，是可以获得全局最优解的轨迹，要学习</li></ul></li></ul><h1 id="实验分析"><a href="#实验分析" class="headerlink" title="实验分析"></a>实验分析</h1><p>作者使用PPO算法在四种类型的任务场景中进行了实验：</p><ol><li>连续动作任务</li><li>分层决策任务</li><li>离散动作任务</li><li>《我的世界》3D构造任务，这个任务主要用来测试SR的可扩展性</li></ol><p>任务的细节设置以及实验结果详情请看原论文。</p><p><img src="./keeping-your-distance-solving-sparse-reward-tasks/experiment1.png" alt=""></p><p>在这两个实验中，可以看出SR比ICM和HER的效果都要好，任务完成的成功率也必将高，且方差比较小。</p><p><img src="./keeping-your-distance-solving-sparse-reward-tasks/experiment2.png" alt=""></p><p>在这个离散的任务中，从图像看出纯DQN即可以解决，但是却没有看到蓝色的纯PPO曲线，按道理来说PPO应该也能解决。</p><p><img src="./keeping-your-distance-solving-sparse-reward-tasks/experiment3.png" alt=""></p><h1 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h1><blockquote><p>We introduce Sibling Rivalry, a simple and effective method for learning goal-reaching tasks from a generic class of distance-based shaped rewards. Sibling Rivalry makes use of sibling rollouts and self-balancing rewards to prevent the learning dynamics from stabilizing around local optima. By leveraging the distance metric used to deﬁne the underlying sparse reward, our technique enables robust learning from shaped rewards without relying on carefully-designed, problem-speciﬁc reward functions. We demonstrate the applicability of our method across a variety of goal-reaching tasks where naive distance-to-goal reward shaping consistently fails and techniques to learn from sparse rewards struggle to explore properly and/or generalize from failed rollouts. Our experiments show that Sibling Rivalry can be readily applied to both continuous and discrete domains, incorporated into hierarchical RL, and scaled to demanding environments.</p></blockquote>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;这篇论文介绍了一个简单有效的model-free方法——&lt;strong&gt;Sibling Rivalry&lt;/strong&gt;(同胞对抗？)，用于解决稀疏奖励问题。该方法特定于“以达到某个目标状态(goal-oriented)”为任务的问题，并且从塑性的距离目标相关奖励(distance-to-goal rewards)中学习。&lt;/p&gt;
&lt;p&gt;推荐：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;self-balancing 奖励机制&lt;/li&gt;
&lt;li&gt;基于奖励函数的创新，比较有趣&lt;/li&gt;
&lt;/ul&gt;
    
    </summary>
    
      <category term="ReinforcementLearning" scheme="http://StepNeverStop.github.io/categories/ReinforcementLearning/"/>
    
    
      <category term="rl" scheme="http://StepNeverStop.github.io/tags/rl/"/>
    
  </entry>
  
  <entry>
    <title>关于安装一些库的问题</title>
    <link href="http://StepNeverStop.github.io/some-issues-of-install-packages.html"/>
    <id>http://StepNeverStop.github.io/some-issues-of-install-packages.html</id>
    <published>2020-03-19T06:30:16.000Z</published>
    <updated>2020-03-19T06:39:10.648Z</updated>
    
    <content type="html"><![CDATA[<p>本篇博客主要用以记录在各种环境安装各种库可能会遇到的问题及其解决方案，以便以后应急查询。</p><a id="more"></a><h1 id="pip"><a href="#pip" class="headerlink" title="pip"></a>pip</h1><h2 id="由于网速不好，导致TimeOut"><a href="#由于网速不好，导致TimeOut" class="headerlink" title="由于网速不好，导致TimeOut"></a>由于网速不好，导致TimeOut</h2><p>这种情况一般是由于pip要安装的库的默认源在国外服务器，导致网速只有几kb。可以考虑使用国内镜像站来安装或更新库。</p><p>比如<code>pip install tensorflow -i [国内镜像站的地址]</code></p><p>更新时也可以用<code>pip install --upgrade tensorflow -i [国内镜像站的地址]</code></p><p>常用的国内镜像站地址有：</p><ul><li>阿里云 <a href="https://mirrors.aliyun.com/pypi/simple/" rel="external nofollow" target="_blank">https://mirrors.aliyun.com/pypi/simple/</a></li><li>中科大 <a href="https://pypi.mirrors.ustc.edu.cn/simple/" rel="external nofollow" target="_blank">https://pypi.mirrors.ustc.edu.cn/simple/</a></li><li>清华 <a href="https://pypi.tuna.tsinghua.edu.cn/simple/" rel="external nofollow" target="_blank">https://pypi.tuna.tsinghua.edu.cn/simple/</a></li><li>豆瓣 <a href="https://pypi.douban.com/simple/" rel="external nofollow" target="_blank">https://pypi.douban.com/simple/</a></li></ul><p>如果报错，可以试试如下命令：</p><p><code>pip install tensorflow -i [国内镜像站的地址] --trusted-host [国内镜像站官网]</code></p><p>比如：</p><p><code>pip install tensorflow -i http://pypi.douban.com/simple --trusted-host pypi.douban.com</code></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本篇博客主要用以记录在各种环境安装各种库可能会遇到的问题及其解决方案，以便以后应急查询。&lt;/p&gt;
    
    </summary>
    
      <category term="小知识" scheme="http://StepNeverStop.github.io/categories/%E5%B0%8F%E7%9F%A5%E8%AF%86/"/>
    
    
      <category term="pip" scheme="http://StepNeverStop.github.io/tags/pip/"/>
    
  </entry>
  
  <entry>
    <title>配置阿里云上的服务器</title>
    <link href="http://StepNeverStop.github.io/config-alios.html"/>
    <id>http://StepNeverStop.github.io/config-alios.html</id>
    <published>2020-01-13T01:22:24.000Z</published>
    <updated>2020-01-16T18:04:24.216Z</updated>
    
    <content type="html"><![CDATA[<p>本文记录了在阿里云服务器上配置自己训练环境的过程。</p><a id="more"></a><h1 id="安装Miniconda"><a href="#安装Miniconda" class="headerlink" title="安装Miniconda"></a>安装Miniconda</h1><p><code>wget https://repo.continuum.io/miniconda/Miniconda3-4.6.14-Linux-x86_64.sh</code></p><p><code>sh Miniconda3-4.6.14-Linux-x86_64.sh</code><br>一路<code>ENTER</code>和<code>yes</code></p><p><code>source ~/.bashrc</code></p><p>更新</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">conda update conda</span><br><span class="line">conda update --all</span><br><span class="line">apt-get update</span><br></pre></td></tr></table></figure><h1 id="安装Git"><a href="#安装Git" class="headerlink" title="安装Git"></a>安装Git</h1><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">apt-get install git</span><br><span class="line">mkdir ~/keavnn</span><br><span class="line">cd ~/keavnn/</span><br><span class="line">git clone https://github.com/StepNeverStop/RLt.git</span><br><span class="line">conda create -n tf2 python=3.6</span><br><span class="line">conda activate tf2</span><br><span class="line">conda install -y docopt numpy pillow yaml pyyaml pandas openpyxl</span><br><span class="line"></span><br><span class="line">apt-get install apt-file</span><br><span class="line">apt-file update</span><br><span class="line">apt-file search libSM.so.6</span><br><span class="line">apt-get install libsm6</span><br><span class="line">apt-file search libXrender.so.1</span><br><span class="line">apt-get install libxrender1</span><br><span class="line"></span><br><span class="line">pip install protobuf grpcio grpcio-tools tensorflow tensorflow_probability</span><br></pre></td></tr></table></figure><h1 id="安装MuJoCo"><a href="#安装MuJoCo" class="headerlink" title="安装MuJoCo"></a>安装MuJoCo</h1><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">mkdir ~/.mujoco</span><br><span class="line">cd .mujoco/</span><br><span class="line">wget url https://www.roboti.us/download/mujoco200_linux.zip</span><br><span class="line">apt-get install zip</span><br><span class="line">unzip mujoco200_linux.zip</span><br><span class="line">wget url https://www.roboti.us/getid/getid_linux</span><br><span class="line">chmod +x getid_linux</span><br><span class="line">./getid_linux</span><br></pre></td></tr></table></figure><p>接着把获取到的计算机ID用于注册30天免费的Mujoco引擎<a href="https://www.roboti.us/license.html" rel="external nofollow" target="_blank">https://www.roboti.us/license.html</a></p><p>把邮箱里的mjkey.txt通过ssh连接或者sftp软件File Zilla放置云服务器上</p><p>放在<code>~/.mujoco</code>与<code>~/.mujoco/mujoco200_linux/bin</code>文件夹下</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">mv ~/.mujoco/mujoco200_linux ~/.mujoco/mujoco200</span><br><span class="line">apt-get install nano</span><br><span class="line">nano ~/.bashrc</span><br></pre></td></tr></table></figure><p>在尾部添加<code>export LD_LIBRARY_PATH=$HOME/.mujoco/mujoco200/bin</code>，<code>export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/root/.mujoco/mujoco200/bin</code>然后保存退出</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">source ~/.bashrc</span><br><span class="line">apt install python3-pip</span><br><span class="line">pip install --upgrade pip</span><br></pre></td></tr></table></figure><p>注意：在配置完Git之后：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">conda activate tf2</span><br><span class="line">apt install libosmesa6-dev</span><br><span class="line">apt-get install python3 python-dev python3-dev \</span><br><span class="line">     build-essential libssl-dev libffi-dev \</span><br><span class="line">     libxml2-dev libxslt1-dev zlib1g-dev \</span><br><span class="line">     python-pip libgl1-mesa-dev patchelf libglfw3 libglfw3-dev</span><br><span class="line">pip install fasteners</span><br><span class="line">pip3 install -i https://pypi.tuna.tsinghua.edu.cn/simple -U &apos;mujoco-py&lt;2.1,&gt;=2.0&apos;</span><br><span class="line"></span><br><span class="line">cd ~/keavnn/RLt/gym</span><br><span class="line">pip install -e &apos;.[all]&apos;</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">Successfully built mujoco-py</span><br><span class="line">Installing collected packages: mujoco-py</span><br><span class="line">Successfully installed mujoco-py-2.0.2.9</span><br></pre></td></tr></table></figure><p>测试一下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">$ python3</span><br><span class="line">import mujoco_py</span><br><span class="line">import os</span><br><span class="line">mj_path, _ = mujoco_py.utils.discover_mujoco()</span><br><span class="line">xml_path = os.path.join(mj_path, &apos;model&apos;, &apos;humanoid.xml&apos;)</span><br><span class="line">model = mujoco_py.load_model_from_path(xml_path)</span><br><span class="line">sim = mujoco_py.MjSim(model)</span><br><span class="line"></span><br><span class="line">print(sim.data.qpos)</span><br><span class="line"># [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]</span><br><span class="line"></span><br><span class="line">sim.step()</span><br><span class="line">print(sim.data.qpos)</span><br><span class="line"># [-2.09531783e-19  2.72130735e-05  6.14480786e-22 -3.45474715e-06</span><br><span class="line">#   7.42993721e-06 -1.40711141e-04 -3.04253586e-04 -2.07559344e-04</span><br><span class="line">#   8.50646247e-05 -3.45474715e-06  7.42993721e-06 -1.40711141e-04</span><br><span class="line">#  -3.04253586e-04 -2.07559344e-04 -8.50646247e-05  1.11317030e-04</span><br><span class="line">#  -7.03465386e-05 -2.22862221e-05 -1.11317030e-04  7.03465386e-05</span><br><span class="line">#  -2.22862221e-05]</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本文记录了在阿里云服务器上配置自己训练环境的过程。&lt;/p&gt;
    
    </summary>
    
      <category term="Docker" scheme="http://StepNeverStop.github.io/categories/Docker/"/>
    
    
      <category term="docker" scheme="http://StepNeverStop.github.io/tags/docker/"/>
    
      <category term="conda" scheme="http://StepNeverStop.github.io/tags/conda/"/>
    
  </entry>
  
  <entry>
    <title>在Windows 10系统上安装gym等环境</title>
    <link href="http://StepNeverStop.github.io/install-atari-and-box2d-on-win10.html"/>
    <id>http://StepNeverStop.github.io/install-atari-and-box2d-on-win10.html</id>
    <published>2019-10-17T07:04:58.000Z</published>
    <updated>2019-10-17T14:27:02.384Z</updated>
    
    <content type="html"><![CDATA[<p>在Win 10系统安装gym，atari，Box2D等环境</p><a id="more"></a><h1 id="环境"><a href="#环境" class="headerlink" title="环境"></a>环境</h1><ul><li>win 10 专业版</li><li>python 3.6</li></ul><h1 id="安装gym"><a href="#安装gym" class="headerlink" title="安装gym"></a>安装gym</h1><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">git <span class="built_in">clone</span> https://github.com/openai/gym</span><br><span class="line"><span class="built_in">cd</span> gym</span><br><span class="line">pip install -e .</span><br></pre></td></tr></table></figure><h1 id="安装atari"><a href="#安装atari" class="headerlink" title="安装atari"></a>安装atari</h1><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install --no-index -f https://github.com/Kojoley/atari-py/releases atari_py</span><br></pre></td></tr></table></figure><h1 id="安装box2d"><a href="#安装box2d" class="headerlink" title="安装box2d"></a>安装box2d</h1><p>在<a href="https://www.lfd.uci.edu/~gohlke/pythonlibs/#pybox2d" rel="external nofollow" target="_blank">https://www.lfd.uci.edu/~gohlke/pythonlibs/#pybox2d</a>下载相应的<code>.whl</code>文件</p><p><img src="./install-atari-and-box2d-on-win10/box2d.png" alt=""></p><p><em>注意分清python版本与操作系统位数</em></p><p>之后再<code>cmd</code>中输入：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">cd</span> download_dir</span><br><span class="line">pip install [name].whl</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;在Win 10系统安装gym，atari，Box2D等环境&lt;/p&gt;
    
    </summary>
    
      <category term="小知识" scheme="http://StepNeverStop.github.io/categories/%E5%B0%8F%E7%9F%A5%E8%AF%86/"/>
    
    
      <category term="rl" scheme="http://StepNeverStop.github.io/tags/rl/"/>
    
      <category term="gym" scheme="http://StepNeverStop.github.io/tags/gym/"/>
    
  </entry>
  
</feed>
